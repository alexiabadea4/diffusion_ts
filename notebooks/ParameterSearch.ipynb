{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from net import GaussianDiffusion\n",
    "from net import EpsilonTheta\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "import math\n",
    "import optuna\n",
    "import os\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to reshape data into [batch size, channels, size].\n",
    "    \"\"\"\n",
    "    # Assuming your signals are originally in the shape [size]\n",
    "    # and you want to add a single channel dimension\n",
    "    signals = torch.stack([item['signals'] for item in batch]).unsqueeze(1)  # Adds a channel dimension\n",
    "    gt = torch.stack([item['gt'] for item in batch])\n",
    "    sc = torch.stack([item['sc'] for item in batch])\n",
    "    \n",
    "    return {'signals': signals, 'gt': gt, 'sc': sc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, train_loader, num_batches_per_epoch, model, optimizer, \n",
    "                validation_iter=None, device='cuda', model_save_path='./model_checkpoints/', \n",
    "                log_interval=100):\n",
    "    \"\"\"\n",
    "    A function to train the model and return the average validation loss.\n",
    "    :param epochs: Number of epochs to train.\n",
    "    :param train_loader: DataLoader for training data.\n",
    "    :param num_batches_per_epoch: Number of batches in each epoch.\n",
    "    :param model: The neural network model to train.\n",
    "    :param optimizer: Optimizer used for training.\n",
    "    :param validation_iter: DataLoader for validation data, if any.\n",
    "    :param device: Device to train on, 'cuda' or 'cpu'.\n",
    "    :param model_save_path: Path to save model checkpoints.\n",
    "    :param log_interval: Interval to log training progress.\n",
    "    \n",
    "    :return: Average validation loss over the validation dataset.\n",
    "    \"\"\"\n",
    "    losses_t = []\n",
    "    if not os.path.exists(model_save_path):\n",
    "        os.makedirs(model_save_path)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        cumm_epoch_loss = 0.0\n",
    "\n",
    "        with tqdm(train_loader, total=num_batches_per_epoch - 1) as it:\n",
    "            for batch_no, data_entry in enumerate(it, start=1):\n",
    "                optimizer.zero_grad()\n",
    "                signals = data_entry['signals'].to(device)\n",
    "                losses = model.log_prob(signals)\n",
    "                cumm_epoch_loss += losses.item()\n",
    "\n",
    "                avg_epoch_loss = cumm_epoch_loss / batch_no\n",
    "                it.set_postfix({\"epoch\": f\"{epoch + 1}/{epochs}\", \"avg_loss\": avg_epoch_loss}, refresh=False)\n",
    "\n",
    "                wandb.log({\"train_loss\": losses.item()})\n",
    "                losses.backward()\n",
    "                optimizer.step()\n",
    "                #lr_scheduler.step()\n",
    "\n",
    "                if num_batches_per_epoch == batch_no:\n",
    "                    break\n",
    "\n",
    "        losses_t.append(avg_epoch_loss)\n",
    "        if (epoch + 1) % log_interval == 0:\n",
    "            model_checkpoint_path = os.path.join(model_save_path, f'model_epoch_{epoch+1}.pth')\n",
    "            torch.save(model.state_dict(), model_checkpoint_path)\n",
    "            print(f'Model saved to {model_checkpoint_path}')\n",
    "\n",
    "    # Validation loop\n",
    "    if validation_iter is not None:\n",
    "        model.eval()\n",
    "        cumm_epoch_loss_val = 0.0\n",
    "        with tqdm(validation_iter, total=num_batches_per_epoch - 1, colour=\"green\") as it:\n",
    "            for batch_no, data_entry in enumerate(it, start=1):\n",
    "                signals = data_entry['signals']\n",
    "                with torch.no_grad():\n",
    "                    losses = model.log_prob(signals)\n",
    "\n",
    "                cumm_epoch_loss_val += losses.item()\n",
    "                avg_epoch_loss_val = cumm_epoch_loss_val / batch_no\n",
    "\n",
    "                it.set_postfix({\"epoch\": f\"{epoch + 1}/{epochs}\", \"avg_val_loss\": avg_epoch_loss_val}, refresh=False)\n",
    "\n",
    "        return avg_epoch_loss_val\n",
    "    else:\n",
    "        return sum(losses_t) / len(losses_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameters to optimize\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    num_layers = trial.suggest_int('num_layers', 4, 16)\n",
    "    residual_channels = trial.suggest_categorical('residual_channels', [16, 32, 64])\n",
    "    dilation_cycle_length = trial.suggest_categorical('dilation_cycle_length', [1, 2, 4])\n",
    "\n",
    "    nb_samples = 10000\n",
    "    num_batches_per_epoch = math.ceil(nb_samples / batch_size)\n",
    "\n",
    "    \n",
    "    \n",
    "    file_path = 'datasets/train_set.pth'\n",
    "    dataset = torch.load(file_path)\n",
    "\n",
    "    # DataLoader for training\n",
    "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True,collate_fn=custom_collate_fn)\n",
    "    # Initialize model, optimizer, etc. using the suggested hyperparameters\n",
    "    # For example:\n",
    "    denoise_fn = EpsilonTheta(target_dim=[256],\n",
    "                              residual_layers=num_layers,\n",
    "                              residual_channels=residual_channels,\n",
    "                              dilation_cycle_length=dilation_cycle_length)\n",
    "    model = GaussianDiffusion(denoise_fn=denoise_fn, input_size=[256], beta_end=0.1, diff_steps=100, loss_type=\"l2\", betas=None, beta_schedule=\"linear\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "\n",
    "    # Your training function should return the validation loss\n",
    "    val_loss = train_model(100, train_loader, num_batches_per_epoch, model, optimizer, \n",
    "                validation_iter=None, device='cpu', model_save_path='./model_checkpoints_optuna/', \n",
    "                log_interval=100)  \n",
    "    \n",
    "    # Log to wandb\n",
    "    wandb.log({'learning_rate': learning_rate,\n",
    "               'batch_size': batch_size,\n",
    "               'num_layers': num_layers,\n",
    "               'residual_channels': residual_channels,\n",
    "               'dilation_cycle_length': dilation_cycle_length,\n",
    "               'val_loss': val_loss})\n",
    "    \n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:28w201fv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e5f8ffdcd634b5595ad93999b2fe6eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>██▄▄▄▂▃▂▃▃▂▃▄▃▂▁▂▄▂▁▂▃▂▃▁▂▂▂▃▂▂▂▂▂▂▂▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>0.07155</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glamorous-firebrand-3</strong> at: <a href='https://wandb.ai/fyp_a/optuna_hyperparameter_optimization/runs/28w201fv' target=\"_blank\">https://wandb.ai/fyp_a/optuna_hyperparameter_optimization/runs/28w201fv</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240303_150615-28w201fv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:28w201fv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Admin\\Desktop\\diffusion_ts\\s\\wandb\\run-20240303_195238-k4zdif76</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fyp_a/optuna_hyperparameter_optimization/runs/k4zdif76' target=\"_blank\">feasible-totem-4</a></strong> to <a href='https://wandb.ai/fyp_a/optuna_hyperparameter_optimization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fyp_a/optuna_hyperparameter_optimization' target=\"_blank\">https://wandb.ai/fyp_a/optuna_hyperparameter_optimization</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fyp_a/optuna_hyperparameter_optimization/runs/k4zdif76' target=\"_blank\">https://wandb.ai/fyp_a/optuna_hyperparameter_optimization/runs/k4zdif76</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/fyp_a/optuna_hyperparameter_optimization/runs/k4zdif76?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2b0b8bbcc08>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"optuna_hyperparameter_optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-03 19:53:04,413] A new study created in memory with name: no-name-469d8df2-0f76-4831-8b0e-70b2ac50953c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\difonedseg\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "100%|██████████| 156/156 [00:42<00:00,  3.69it/s, epoch=1/100, avg_loss=0.606]\n",
      "100%|██████████| 156/156 [00:46<00:00,  3.33it/s, epoch=2/100, avg_loss=0.254]\n",
      "100%|██████████| 156/156 [00:40<00:00,  3.83it/s, epoch=3/100, avg_loss=0.223]\n",
      "100%|██████████| 156/156 [00:41<00:00,  3.74it/s, epoch=4/100, avg_loss=0.2]  \n",
      "100%|██████████| 156/156 [00:42<00:00,  3.66it/s, epoch=5/100, avg_loss=0.179]\n",
      "100%|██████████| 156/156 [00:36<00:00,  4.29it/s, epoch=6/100, avg_loss=0.167]\n",
      "100%|██████████| 156/156 [00:42<00:00,  3.70it/s, epoch=7/100, avg_loss=0.151]\n",
      "100%|██████████| 156/156 [00:41<00:00,  3.72it/s, epoch=8/100, avg_loss=0.149]\n",
      "100%|██████████| 156/156 [00:43<00:00,  3.55it/s, epoch=9/100, avg_loss=0.133]\n",
      "100%|██████████| 156/156 [00:43<00:00,  3.58it/s, epoch=10/100, avg_loss=0.127]\n",
      "100%|██████████| 156/156 [00:43<00:00,  3.61it/s, epoch=11/100, avg_loss=0.125]\n",
      "100%|██████████| 156/156 [00:41<00:00,  3.79it/s, epoch=12/100, avg_loss=0.117]\n",
      "100%|██████████| 156/156 [00:43<00:00,  3.59it/s, epoch=13/100, avg_loss=0.113]\n",
      "100%|██████████| 156/156 [00:42<00:00,  3.65it/s, epoch=14/100, avg_loss=0.112]\n",
      "100%|██████████| 156/156 [00:39<00:00,  3.92it/s, epoch=15/100, avg_loss=0.106]\n",
      "100%|██████████| 156/156 [00:38<00:00,  4.04it/s, epoch=16/100, avg_loss=0.105]\n",
      "100%|██████████| 156/156 [00:38<00:00,  4.06it/s, epoch=17/100, avg_loss=0.106]\n",
      "100%|██████████| 156/156 [00:38<00:00,  4.10it/s, epoch=18/100, avg_loss=0.101] \n",
      "100%|██████████| 156/156 [00:38<00:00,  4.05it/s, epoch=19/100, avg_loss=0.101]\n",
      "100%|██████████| 156/156 [00:38<00:00,  4.01it/s, epoch=20/100, avg_loss=0.0973]\n",
      "100%|██████████| 156/156 [00:38<00:00,  4.06it/s, epoch=21/100, avg_loss=0.0959]\n",
      "100%|██████████| 156/156 [00:42<00:00,  3.65it/s, epoch=22/100, avg_loss=0.0956]\n",
      "100%|██████████| 156/156 [00:46<00:00,  3.35it/s, epoch=23/100, avg_loss=0.0954]\n",
      "100%|██████████| 156/156 [00:49<00:00,  3.14it/s, epoch=24/100, avg_loss=0.0943]\n",
      "100%|██████████| 156/156 [00:45<00:00,  3.43it/s, epoch=25/100, avg_loss=0.093] \n",
      "100%|██████████| 156/156 [00:41<00:00,  3.77it/s, epoch=26/100, avg_loss=0.0945]\n",
      "100%|██████████| 156/156 [00:38<00:00,  4.09it/s, epoch=27/100, avg_loss=0.0934]\n",
      "100%|██████████| 156/156 [00:39<00:00,  3.92it/s, epoch=28/100, avg_loss=0.0921]\n",
      "100%|██████████| 156/156 [00:38<00:00,  4.09it/s, epoch=29/100, avg_loss=0.0906]\n",
      "100%|██████████| 156/156 [00:39<00:00,  3.99it/s, epoch=30/100, avg_loss=0.0904]\n",
      "100%|██████████| 156/156 [00:37<00:00,  4.11it/s, epoch=31/100, avg_loss=0.0897]\n",
      "100%|██████████| 156/156 [00:38<00:00,  4.06it/s, epoch=32/100, avg_loss=0.0902]\n",
      "100%|██████████| 156/156 [00:39<00:00,  3.99it/s, epoch=33/100, avg_loss=0.0891]\n",
      "100%|██████████| 156/156 [00:42<00:00,  3.69it/s, epoch=34/100, avg_loss=0.0895]\n",
      "100%|██████████| 156/156 [00:39<00:00,  3.99it/s, epoch=35/100, avg_loss=0.0884]\n",
      "100%|██████████| 156/156 [00:37<00:00,  4.18it/s, epoch=36/100, avg_loss=0.0888]\n",
      "100%|██████████| 156/156 [00:51<00:00,  3.01it/s, epoch=37/100, avg_loss=0.0875]\n",
      "100%|██████████| 156/156 [00:47<00:00,  3.27it/s, epoch=38/100, avg_loss=0.0875]\n",
      "100%|██████████| 156/156 [00:47<00:00,  3.30it/s, epoch=39/100, avg_loss=0.0873]\n",
      "100%|██████████| 156/156 [00:49<00:00,  3.13it/s, epoch=40/100, avg_loss=0.0864]\n",
      "100%|██████████| 156/156 [01:05<00:00,  2.37it/s, epoch=41/100, avg_loss=0.0856]\n",
      "100%|██████████| 156/156 [00:46<00:00,  3.38it/s, epoch=42/100, avg_loss=0.0863]\n",
      "100%|██████████| 156/156 [00:47<00:00,  3.32it/s, epoch=43/100, avg_loss=0.0865]\n",
      "100%|██████████| 156/156 [00:48<00:00,  3.18it/s, epoch=44/100, avg_loss=0.0862]\n",
      "100%|██████████| 156/156 [00:50<00:00,  3.10it/s, epoch=45/100, avg_loss=0.0842]\n",
      "100%|██████████| 156/156 [00:47<00:00,  3.31it/s, epoch=46/100, avg_loss=0.0853]\n",
      "100%|██████████| 156/156 [00:35<00:00,  4.34it/s, epoch=47/100, avg_loss=0.0826]\n",
      "100%|██████████| 156/156 [00:36<00:00,  4.28it/s, epoch=48/100, avg_loss=0.0843]\n",
      "100%|██████████| 156/156 [00:48<00:00,  3.25it/s, epoch=49/100, avg_loss=0.0842]\n",
      "100%|██████████| 156/156 [00:50<00:00,  3.08it/s, epoch=50/100, avg_loss=0.0839]\n",
      "100%|██████████| 156/156 [00:47<00:00,  3.31it/s, epoch=51/100, avg_loss=0.0845]\n",
      "100%|██████████| 156/156 [00:41<00:00,  3.77it/s, epoch=52/100, avg_loss=0.0848]\n",
      "100%|██████████| 156/156 [00:35<00:00,  4.44it/s, epoch=53/100, avg_loss=0.0823]\n",
      "100%|██████████| 156/156 [00:38<00:00,  4.07it/s, epoch=54/100, avg_loss=0.084] \n",
      "100%|██████████| 156/156 [00:38<00:00,  4.05it/s, epoch=55/100, avg_loss=0.0836]\n",
      "100%|██████████| 156/156 [00:46<00:00,  3.36it/s, epoch=56/100, avg_loss=0.0831]\n",
      "100%|██████████| 156/156 [00:44<00:00,  3.51it/s, epoch=57/100, avg_loss=0.0826]\n",
      "100%|██████████| 156/156 [00:41<00:00,  3.77it/s, epoch=58/100, avg_loss=0.0816]\n",
      "100%|██████████| 156/156 [00:34<00:00,  4.48it/s, epoch=59/100, avg_loss=0.0835]\n",
      "100%|██████████| 156/156 [00:36<00:00,  4.28it/s, epoch=60/100, avg_loss=0.0831]\n",
      "100%|██████████| 156/156 [00:38<00:00,  4.09it/s, epoch=61/100, avg_loss=0.0836]\n",
      "100%|██████████| 156/156 [00:41<00:00,  3.73it/s, epoch=62/100, avg_loss=0.0819]\n",
      "100%|██████████| 156/156 [00:35<00:00,  4.40it/s, epoch=63/100, avg_loss=0.0816]\n",
      "100%|██████████| 156/156 [00:37<00:00,  4.12it/s, epoch=64/100, avg_loss=0.0802]\n",
      "100%|██████████| 156/156 [00:39<00:00,  3.98it/s, epoch=65/100, avg_loss=0.0806]\n",
      "100%|██████████| 156/156 [00:40<00:00,  3.82it/s, epoch=66/100, avg_loss=0.082] \n",
      "100%|██████████| 156/156 [01:18<00:00,  1.98it/s, epoch=67/100, avg_loss=0.0812]\n",
      "100%|██████████| 156/156 [00:42<00:00,  3.70it/s, epoch=68/100, avg_loss=0.0808]\n",
      "100%|██████████| 156/156 [00:36<00:00,  4.30it/s, epoch=69/100, avg_loss=0.0812]\n",
      "100%|██████████| 156/156 [00:35<00:00,  4.42it/s, epoch=70/100, avg_loss=0.0794]\n",
      "100%|██████████| 156/156 [00:36<00:00,  4.32it/s, epoch=71/100, avg_loss=0.0805]\n",
      "100%|██████████| 156/156 [00:35<00:00,  4.40it/s, epoch=72/100, avg_loss=0.0799]\n",
      "100%|██████████| 156/156 [00:35<00:00,  4.33it/s, epoch=73/100, avg_loss=0.0796]\n",
      "100%|██████████| 156/156 [00:35<00:00,  4.42it/s, epoch=74/100, avg_loss=0.0809]\n",
      "100%|██████████| 156/156 [00:35<00:00,  4.38it/s, epoch=75/100, avg_loss=0.0803]\n",
      "100%|██████████| 156/156 [00:34<00:00,  4.48it/s, epoch=76/100, avg_loss=0.0796]\n",
      "100%|██████████| 156/156 [00:34<00:00,  4.52it/s, epoch=77/100, avg_loss=0.0803]\n",
      "100%|██████████| 156/156 [00:33<00:00,  4.67it/s, epoch=78/100, avg_loss=0.0807]\n",
      "100%|██████████| 156/156 [00:32<00:00,  4.73it/s, epoch=79/100, avg_loss=0.0796]\n",
      "100%|██████████| 156/156 [00:32<00:00,  4.77it/s, epoch=80/100, avg_loss=0.0801]\n",
      "100%|██████████| 156/156 [00:33<00:00,  4.71it/s, epoch=81/100, avg_loss=0.0791]\n",
      "100%|██████████| 156/156 [00:33<00:00,  4.67it/s, epoch=82/100, avg_loss=0.0796]\n",
      "100%|██████████| 156/156 [00:32<00:00,  4.73it/s, epoch=83/100, avg_loss=0.0794]\n",
      "100%|██████████| 156/156 [00:33<00:00,  4.63it/s, epoch=84/100, avg_loss=0.08]  \n",
      "100%|██████████| 156/156 [00:40<00:00,  3.86it/s, epoch=85/100, avg_loss=0.0794]\n",
      "100%|██████████| 156/156 [00:39<00:00,  3.94it/s, epoch=86/100, avg_loss=0.0807]\n",
      "100%|██████████| 156/156 [00:39<00:00,  3.94it/s, epoch=87/100, avg_loss=0.0796]\n",
      "100%|██████████| 156/156 [00:40<00:00,  3.90it/s, epoch=88/100, avg_loss=0.0781]\n",
      "100%|██████████| 156/156 [00:32<00:00,  4.85it/s, epoch=89/100, avg_loss=0.0783]\n",
      "100%|██████████| 156/156 [00:40<00:00,  3.82it/s, epoch=90/100, avg_loss=0.0785]\n",
      "100%|██████████| 156/156 [00:43<00:00,  3.55it/s, epoch=91/100, avg_loss=0.0792]\n",
      "100%|██████████| 156/156 [00:44<00:00,  3.49it/s, epoch=92/100, avg_loss=0.078] \n",
      "100%|██████████| 156/156 [00:43<00:00,  3.55it/s, epoch=93/100, avg_loss=0.0801]\n",
      "100%|██████████| 156/156 [00:44<00:00,  3.54it/s, epoch=94/100, avg_loss=0.0784]\n",
      "100%|██████████| 156/156 [00:44<00:00,  3.52it/s, epoch=95/100, avg_loss=0.0785]\n",
      "100%|██████████| 156/156 [00:37<00:00,  4.11it/s, epoch=96/100, avg_loss=0.079] \n",
      "100%|██████████| 156/156 [00:44<00:00,  3.53it/s, epoch=97/100, avg_loss=0.0781]\n",
      "100%|██████████| 156/156 [00:44<00:00,  3.50it/s, epoch=98/100, avg_loss=0.0777]\n",
      "100%|██████████| 156/156 [00:38<00:00,  4.04it/s, epoch=99/100, avg_loss=0.0769]\n",
      "100%|██████████| 156/156 [00:40<00:00,  3.83it/s, epoch=100/100, avg_loss=0.077] \n",
      "[I 2024-03-03 21:01:46,796] Trial 0 finished with value: 0.09964051761872071 and parameters: {'learning_rate': 9.945970661512229e-05, 'batch_size': 64, 'num_layers': 14, 'residual_channels': 16, 'dilation_cycle_length': 1}. Best is trial 0 with value: 0.09964051761872071.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./model_checkpoints_optuna/model_epoch_100.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [00:38<00:00,  4.01it/s, epoch=1/100, avg_loss=0.266]\n",
      "100%|██████████| 156/156 [00:42<00:00,  3.68it/s, epoch=2/100, avg_loss=0.151]\n",
      "100%|██████████| 156/156 [00:43<00:00,  3.60it/s, epoch=3/100, avg_loss=0.126]\n",
      "100%|██████████| 156/156 [00:51<00:00,  3.05it/s, epoch=4/100, avg_loss=0.112]\n",
      "100%|██████████| 156/156 [00:54<00:00,  2.87it/s, epoch=5/100, avg_loss=0.103]\n",
      "100%|██████████| 156/156 [00:48<00:00,  3.20it/s, epoch=6/100, avg_loss=0.0955]\n",
      "100%|██████████| 156/156 [00:48<00:00,  3.19it/s, epoch=7/100, avg_loss=0.0925]\n",
      "100%|██████████| 156/156 [00:40<00:00,  3.81it/s, epoch=8/100, avg_loss=0.0903]\n",
      "100%|██████████| 156/156 [00:47<00:00,  3.28it/s, epoch=9/100, avg_loss=0.0888]\n",
      "100%|██████████| 156/156 [00:50<00:00,  3.10it/s, epoch=10/100, avg_loss=0.09]  \n",
      "100%|██████████| 156/156 [00:46<00:00,  3.34it/s, epoch=11/100, avg_loss=0.0864]\n",
      "100%|██████████| 156/156 [00:46<00:00,  3.32it/s, epoch=12/100, avg_loss=0.0861]\n",
      "100%|██████████| 156/156 [00:48<00:00,  3.21it/s, epoch=13/100, avg_loss=0.0852]\n",
      "100%|██████████| 156/156 [00:49<00:00,  3.16it/s, epoch=14/100, avg_loss=0.0839]\n",
      "100%|██████████| 156/156 [00:50<00:00,  3.10it/s, epoch=15/100, avg_loss=0.0846]\n",
      "100%|██████████| 156/156 [00:46<00:00,  3.38it/s, epoch=16/100, avg_loss=0.083] \n",
      "100%|██████████| 156/156 [00:50<00:00,  3.08it/s, epoch=17/100, avg_loss=0.0804]\n",
      "100%|██████████| 156/156 [00:50<00:00,  3.09it/s, epoch=18/100, avg_loss=0.0812]\n",
      "100%|██████████| 156/156 [00:42<00:00,  3.64it/s, epoch=19/100, avg_loss=0.0809]\n",
      "100%|██████████| 156/156 [00:47<00:00,  3.27it/s, epoch=20/100, avg_loss=0.0807]\n",
      "100%|██████████| 156/156 [00:52<00:00,  2.95it/s, epoch=21/100, avg_loss=0.0798]\n",
      "100%|██████████| 156/156 [00:49<00:00,  3.18it/s, epoch=22/100, avg_loss=0.0804]\n",
      "100%|██████████| 156/156 [00:46<00:00,  3.32it/s, epoch=23/100, avg_loss=0.0795]\n",
      "100%|██████████| 156/156 [00:49<00:00,  3.16it/s, epoch=24/100, avg_loss=0.0779]\n",
      "100%|██████████| 156/156 [00:53<00:00,  2.92it/s, epoch=25/100, avg_loss=0.0778]\n",
      "100%|██████████| 156/156 [00:54<00:00,  2.88it/s, epoch=26/100, avg_loss=0.0763]\n",
      "100%|██████████| 156/156 [00:45<00:00,  3.42it/s, epoch=27/100, avg_loss=0.0779]\n",
      "100%|██████████| 156/156 [00:51<00:00,  3.02it/s, epoch=28/100, avg_loss=0.0784]\n",
      "100%|██████████| 156/156 [00:52<00:00,  2.98it/s, epoch=29/100, avg_loss=0.076] \n",
      "100%|██████████| 156/156 [01:04<00:00,  2.43it/s, epoch=30/100, avg_loss=0.076] \n",
      "100%|██████████| 156/156 [00:41<00:00,  3.73it/s, epoch=31/100, avg_loss=0.0761]\n",
      "100%|██████████| 156/156 [00:53<00:00,  2.90it/s, epoch=32/100, avg_loss=0.0746]\n",
      "100%|██████████| 156/156 [00:52<00:00,  2.99it/s, epoch=33/100, avg_loss=0.0761]\n",
      "100%|██████████| 156/156 [00:39<00:00,  3.92it/s, epoch=34/100, avg_loss=0.0749]\n",
      "100%|██████████| 156/156 [00:39<00:00,  3.98it/s, epoch=35/100, avg_loss=0.076] \n",
      "100%|██████████| 156/156 [00:37<00:00,  4.12it/s, epoch=36/100, avg_loss=0.0738]\n",
      "100%|██████████| 156/156 [00:37<00:00,  4.12it/s, epoch=37/100, avg_loss=0.0743]\n",
      "100%|██████████| 156/156 [00:37<00:00,  4.16it/s, epoch=38/100, avg_loss=0.0726]\n",
      "100%|██████████| 156/156 [00:38<00:00,  4.09it/s, epoch=39/100, avg_loss=0.0741]\n",
      "100%|██████████| 156/156 [00:40<00:00,  3.82it/s, epoch=40/100, avg_loss=0.0729]\n",
      "100%|██████████| 156/156 [00:46<00:00,  3.38it/s, epoch=41/100, avg_loss=0.0721]\n",
      "100%|██████████| 156/156 [00:46<00:00,  3.34it/s, epoch=42/100, avg_loss=0.0729]\n",
      "100%|██████████| 156/156 [00:58<00:00,  2.69it/s, epoch=43/100, avg_loss=0.0736]\n",
      "100%|██████████| 156/156 [00:56<00:00,  2.76it/s, epoch=44/100, avg_loss=0.0716]\n",
      "100%|██████████| 156/156 [01:00<00:00,  2.59it/s, epoch=45/100, avg_loss=0.0729]\n",
      "100%|██████████| 156/156 [00:52<00:00,  2.99it/s, epoch=46/100, avg_loss=0.072] \n",
      "100%|██████████| 156/156 [00:45<00:00,  3.42it/s, epoch=47/100, avg_loss=0.0715]\n",
      "100%|██████████| 156/156 [00:49<00:00,  3.18it/s, epoch=48/100, avg_loss=0.0712]\n",
      "100%|██████████| 156/156 [01:00<00:00,  2.58it/s, epoch=49/100, avg_loss=0.0714]\n",
      "100%|██████████| 156/156 [00:44<00:00,  3.49it/s, epoch=50/100, avg_loss=0.07]  \n",
      "100%|██████████| 156/156 [00:52<00:00,  3.00it/s, epoch=51/100, avg_loss=0.0709]\n",
      "100%|██████████| 156/156 [00:41<00:00,  3.74it/s, epoch=52/100, avg_loss=0.0711]\n",
      "100%|██████████| 156/156 [00:41<00:00,  3.75it/s, epoch=53/100, avg_loss=0.0695]\n",
      "100%|██████████| 156/156 [00:38<00:00,  4.09it/s, epoch=54/100, avg_loss=0.0701]\n",
      "100%|██████████| 156/156 [00:43<00:00,  3.58it/s, epoch=55/100, avg_loss=0.0709]\n",
      "100%|██████████| 156/156 [00:47<00:00,  3.31it/s, epoch=56/100, avg_loss=0.0704]\n",
      "100%|██████████| 156/156 [00:48<00:00,  3.25it/s, epoch=57/100, avg_loss=0.069] \n",
      "100%|██████████| 156/156 [00:48<00:00,  3.19it/s, epoch=58/100, avg_loss=0.0692]\n",
      "100%|██████████| 156/156 [00:34<00:00,  4.57it/s, epoch=59/100, avg_loss=0.0689]\n",
      "100%|██████████| 156/156 [00:35<00:00,  4.42it/s, epoch=60/100, avg_loss=0.0703]\n",
      "100%|██████████| 156/156 [00:37<00:00,  4.11it/s, epoch=61/100, avg_loss=0.0703]\n",
      "100%|██████████| 156/156 [00:33<00:00,  4.64it/s, epoch=62/100, avg_loss=0.0687]\n",
      "100%|██████████| 156/156 [00:33<00:00,  4.67it/s, epoch=63/100, avg_loss=0.0701]\n",
      "100%|██████████| 156/156 [00:34<00:00,  4.58it/s, epoch=64/100, avg_loss=0.069] \n",
      "100%|██████████| 156/156 [00:42<00:00,  3.69it/s, epoch=65/100, avg_loss=0.0687]\n",
      "100%|██████████| 156/156 [00:33<00:00,  4.64it/s, epoch=66/100, avg_loss=0.0683]\n",
      "100%|██████████| 156/156 [00:33<00:00,  4.67it/s, epoch=67/100, avg_loss=0.0685]\n",
      "100%|██████████| 156/156 [00:33<00:00,  4.63it/s, epoch=68/100, avg_loss=0.0693]\n",
      "100%|██████████| 156/156 [00:33<00:00,  4.65it/s, epoch=69/100, avg_loss=0.0697]\n",
      "100%|██████████| 156/156 [00:33<00:00,  4.62it/s, epoch=70/100, avg_loss=0.0685]\n",
      "100%|██████████| 156/156 [00:33<00:00,  4.62it/s, epoch=71/100, avg_loss=0.0683]\n",
      "100%|██████████| 156/156 [00:34<00:00,  4.46it/s, epoch=72/100, avg_loss=0.0679]\n",
      "100%|██████████| 156/156 [00:49<00:00,  3.17it/s, epoch=73/100, avg_loss=0.068] \n",
      "100%|██████████| 156/156 [00:52<00:00,  2.97it/s, epoch=74/100, avg_loss=0.0681]\n",
      "100%|██████████| 156/156 [00:42<00:00,  3.65it/s, epoch=75/100, avg_loss=0.0689]\n",
      "100%|██████████| 156/156 [00:52<00:00,  2.97it/s, epoch=76/100, avg_loss=0.0679]\n",
      "100%|██████████| 156/156 [00:43<00:00,  3.59it/s, epoch=77/100, avg_loss=0.0682]\n",
      "100%|██████████| 156/156 [00:41<00:00,  3.79it/s, epoch=78/100, avg_loss=0.0679]\n",
      "100%|██████████| 156/156 [00:43<00:00,  3.59it/s, epoch=79/100, avg_loss=0.0678]\n",
      "100%|██████████| 156/156 [00:54<00:00,  2.84it/s, epoch=80/100, avg_loss=0.0672]\n",
      "100%|██████████| 156/156 [00:48<00:00,  3.25it/s, epoch=81/100, avg_loss=0.0686]\n",
      "100%|██████████| 156/156 [00:59<00:00,  2.63it/s, epoch=82/100, avg_loss=0.0677]\n",
      "100%|██████████| 156/156 [00:53<00:00,  2.91it/s, epoch=83/100, avg_loss=0.0686]\n",
      "100%|██████████| 156/156 [00:51<00:00,  3.04it/s, epoch=84/100, avg_loss=0.0664]\n",
      "100%|██████████| 156/156 [00:49<00:00,  3.12it/s, epoch=85/100, avg_loss=0.0658]\n",
      "100%|██████████| 156/156 [00:45<00:00,  3.45it/s, epoch=86/100, avg_loss=0.0677]\n",
      "100%|██████████| 156/156 [00:42<00:00,  3.71it/s, epoch=87/100, avg_loss=0.0678]\n",
      "100%|██████████| 156/156 [00:41<00:00,  3.77it/s, epoch=88/100, avg_loss=0.0669]\n",
      "100%|██████████| 156/156 [00:35<00:00,  4.41it/s, epoch=89/100, avg_loss=0.0672]\n",
      "100%|██████████| 156/156 [00:36<00:00,  4.27it/s, epoch=90/100, avg_loss=0.0667]\n",
      "100%|██████████| 156/156 [00:36<00:00,  4.32it/s, epoch=91/100, avg_loss=0.0684]\n",
      "100%|██████████| 156/156 [00:48<00:00,  3.23it/s, epoch=92/100, avg_loss=0.0668]\n",
      "100%|██████████| 156/156 [00:40<00:00,  3.90it/s, epoch=93/100, avg_loss=0.0684]\n",
      "100%|██████████| 156/156 [00:37<00:00,  4.16it/s, epoch=94/100, avg_loss=0.0657]\n",
      "100%|██████████| 156/156 [00:36<00:00,  4.29it/s, epoch=95/100, avg_loss=0.0672]\n",
      "100%|██████████| 156/156 [00:36<00:00,  4.23it/s, epoch=96/100, avg_loss=0.0664]\n",
      "100%|██████████| 156/156 [00:36<00:00,  4.23it/s, epoch=97/100, avg_loss=0.0665]\n",
      "100%|██████████| 156/156 [00:38<00:00,  4.02it/s, epoch=98/100, avg_loss=0.0668]\n",
      "100%|██████████| 156/156 [00:40<00:00,  3.86it/s, epoch=99/100, avg_loss=0.0665]\n",
      "100%|██████████| 156/156 [00:38<00:00,  4.05it/s, epoch=100/100, avg_loss=0.0661]\n",
      "[I 2024-03-03 22:16:34,159] Trial 1 finished with value: 0.07706547251979637 and parameters: {'learning_rate': 0.0007153912999894027, 'batch_size': 64, 'num_layers': 15, 'residual_channels': 16, 'dilation_cycle_length': 2}. Best is trial 1 with value: 0.07706547251979637.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./model_checkpoints_optuna/model_epoch_100.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [00:46<00:00,  3.33it/s, epoch=1/100, avg_loss=1.05]\n",
      "100%|██████████| 156/156 [00:45<00:00,  3.42it/s, epoch=2/100, avg_loss=0.787]\n",
      "100%|██████████| 156/156 [00:47<00:00,  3.31it/s, epoch=3/100, avg_loss=0.639]\n",
      "100%|██████████| 156/156 [00:44<00:00,  3.51it/s, epoch=4/100, avg_loss=0.511]\n",
      "100%|██████████| 156/156 [00:38<00:00,  4.02it/s, epoch=5/100, avg_loss=0.412]\n",
      "100%|██████████| 156/156 [00:34<00:00,  4.51it/s, epoch=6/100, avg_loss=0.335]\n",
      "100%|██████████| 156/156 [00:35<00:00,  4.44it/s, epoch=7/100, avg_loss=0.279]\n",
      "100%|██████████| 156/156 [00:36<00:00,  4.25it/s, epoch=8/100, avg_loss=0.254]\n",
      "100%|██████████| 156/156 [00:40<00:00,  3.88it/s, epoch=9/100, avg_loss=0.237]\n",
      "100%|██████████| 156/156 [00:38<00:00,  4.04it/s, epoch=10/100, avg_loss=0.232]\n",
      "100%|██████████| 156/156 [00:47<00:00,  3.30it/s, epoch=11/100, avg_loss=0.228]\n",
      "100%|██████████| 156/156 [00:46<00:00,  3.35it/s, epoch=12/100, avg_loss=0.231]\n",
      "100%|██████████| 156/156 [00:43<00:00,  3.58it/s, epoch=13/100, avg_loss=0.222]\n",
      "100%|██████████| 156/156 [00:47<00:00,  3.26it/s, epoch=14/100, avg_loss=0.212]\n",
      "100%|██████████| 156/156 [00:40<00:00,  3.88it/s, epoch=15/100, avg_loss=0.209]\n",
      "100%|██████████| 156/156 [00:39<00:00,  3.96it/s, epoch=16/100, avg_loss=0.206]\n",
      "100%|██████████| 156/156 [00:42<00:00,  3.65it/s, epoch=17/100, avg_loss=0.203]\n",
      "100%|██████████| 156/156 [00:43<00:00,  3.61it/s, epoch=18/100, avg_loss=0.203]\n",
      "100%|██████████| 156/156 [00:40<00:00,  3.83it/s, epoch=19/100, avg_loss=0.192]\n",
      "100%|██████████| 156/156 [00:37<00:00,  4.21it/s, epoch=20/100, avg_loss=0.191]\n",
      "100%|██████████| 156/156 [00:41<00:00,  3.79it/s, epoch=21/100, avg_loss=0.189]\n",
      "100%|██████████| 156/156 [00:50<00:00,  3.11it/s, epoch=22/100, avg_loss=0.178]\n",
      "100%|██████████| 156/156 [00:45<00:00,  3.40it/s, epoch=23/100, avg_loss=0.179]\n",
      "100%|██████████| 156/156 [00:46<00:00,  3.39it/s, epoch=24/100, avg_loss=0.178]\n",
      "100%|██████████| 156/156 [00:40<00:00,  3.81it/s, epoch=25/100, avg_loss=0.168]\n",
      "100%|██████████| 156/156 [00:42<00:00,  3.66it/s, epoch=26/100, avg_loss=0.163]\n",
      "100%|██████████| 156/156 [00:46<00:00,  3.39it/s, epoch=27/100, avg_loss=0.162]\n",
      "100%|██████████| 156/156 [00:41<00:00,  3.75it/s, epoch=28/100, avg_loss=0.161]\n",
      "100%|██████████| 156/156 [00:41<00:00,  3.74it/s, epoch=29/100, avg_loss=0.157]\n",
      "100%|██████████| 156/156 [00:41<00:00,  3.74it/s, epoch=30/100, avg_loss=0.151]\n",
      "100%|██████████| 156/156 [00:41<00:00,  3.74it/s, epoch=31/100, avg_loss=0.15] \n",
      "100%|██████████| 156/156 [00:41<00:00,  3.75it/s, epoch=32/100, avg_loss=0.15] \n",
      "100%|██████████| 156/156 [00:50<00:00,  3.08it/s, epoch=33/100, avg_loss=0.15] \n",
      "100%|██████████| 156/156 [00:50<00:00,  3.10it/s, epoch=34/100, avg_loss=0.146]\n",
      "100%|██████████| 156/156 [00:48<00:00,  3.20it/s, epoch=35/100, avg_loss=0.146]\n",
      "100%|██████████| 156/156 [00:47<00:00,  3.30it/s, epoch=36/100, avg_loss=0.135]\n",
      "100%|██████████| 156/156 [00:42<00:00,  3.69it/s, epoch=37/100, avg_loss=0.138]\n",
      "100%|██████████| 156/156 [00:41<00:00,  3.78it/s, epoch=38/100, avg_loss=0.137]\n",
      "100%|██████████| 156/156 [00:41<00:00,  3.79it/s, epoch=39/100, avg_loss=0.131]\n",
      "100%|██████████| 156/156 [00:41<00:00,  3.75it/s, epoch=40/100, avg_loss=0.135]\n",
      "100%|██████████| 156/156 [00:46<00:00,  3.36it/s, epoch=41/100, avg_loss=0.133]\n",
      "100%|██████████| 156/156 [00:49<00:00,  3.17it/s, epoch=42/100, avg_loss=0.13] \n",
      "100%|██████████| 156/156 [00:47<00:00,  3.30it/s, epoch=43/100, avg_loss=0.128]\n",
      "100%|██████████| 156/156 [00:53<00:00,  2.94it/s, epoch=44/100, avg_loss=0.129]\n",
      "100%|██████████| 156/156 [00:43<00:00,  3.59it/s, epoch=45/100, avg_loss=0.127]\n",
      "100%|██████████| 156/156 [00:43<00:00,  3.60it/s, epoch=46/100, avg_loss=0.124]\n",
      "100%|██████████| 156/156 [00:44<00:00,  3.51it/s, epoch=47/100, avg_loss=0.123]\n",
      "100%|██████████| 156/156 [00:43<00:00,  3.59it/s, epoch=48/100, avg_loss=0.122]\n",
      "100%|██████████| 156/156 [00:46<00:00,  3.35it/s, epoch=49/100, avg_loss=0.122]\n",
      "100%|██████████| 156/156 [00:40<00:00,  3.86it/s, epoch=50/100, avg_loss=0.118]\n",
      "100%|██████████| 156/156 [00:44<00:00,  3.53it/s, epoch=51/100, avg_loss=0.115]\n",
      "100%|██████████| 156/156 [00:42<00:00,  3.65it/s, epoch=52/100, avg_loss=0.116]\n",
      "100%|██████████| 156/156 [00:43<00:00,  3.55it/s, epoch=53/100, avg_loss=0.116]\n",
      "100%|██████████| 156/156 [00:40<00:00,  3.82it/s, epoch=54/100, avg_loss=0.114]\n",
      "100%|██████████| 156/156 [00:40<00:00,  3.86it/s, epoch=55/100, avg_loss=0.115]\n",
      "100%|██████████| 156/156 [00:41<00:00,  3.75it/s, epoch=56/100, avg_loss=0.113]\n",
      "100%|██████████| 156/156 [00:57<00:00,  2.69it/s, epoch=57/100, avg_loss=0.112]\n",
      "100%|██████████| 156/156 [00:43<00:00,  3.57it/s, epoch=58/100, avg_loss=0.11] \n",
      "100%|██████████| 156/156 [00:35<00:00,  4.40it/s, epoch=59/100, avg_loss=0.109]\n",
      "100%|██████████| 156/156 [00:35<00:00,  4.34it/s, epoch=60/100, avg_loss=0.112]\n",
      "100%|██████████| 156/156 [00:56<00:00,  2.74it/s, epoch=61/100, avg_loss=0.11] \n",
      "100%|██████████| 156/156 [00:55<00:00,  2.80it/s, epoch=62/100, avg_loss=0.108]\n",
      "100%|██████████| 156/156 [00:53<00:00,  2.93it/s, epoch=63/100, avg_loss=0.11] \n",
      "100%|██████████| 156/156 [00:43<00:00,  3.61it/s, epoch=64/100, avg_loss=0.109]\n",
      "100%|██████████| 156/156 [00:42<00:00,  3.71it/s, epoch=65/100, avg_loss=0.105]\n",
      "100%|██████████| 156/156 [00:40<00:00,  3.88it/s, epoch=66/100, avg_loss=0.107]\n",
      "100%|██████████| 156/156 [00:38<00:00,  4.02it/s, epoch=67/100, avg_loss=0.104]\n",
      "100%|██████████| 156/156 [00:37<00:00,  4.17it/s, epoch=68/100, avg_loss=0.103]\n",
      "100%|██████████| 156/156 [00:38<00:00,  4.03it/s, epoch=69/100, avg_loss=0.104]\n",
      "100%|██████████| 156/156 [00:38<00:00,  4.04it/s, epoch=70/100, avg_loss=0.105]\n",
      "100%|██████████| 156/156 [00:34<00:00,  4.56it/s, epoch=71/100, avg_loss=0.103]\n",
      "100%|██████████| 156/156 [00:34<00:00,  4.50it/s, epoch=72/100, avg_loss=0.103]\n",
      "100%|██████████| 156/156 [00:35<00:00,  4.43it/s, epoch=73/100, avg_loss=0.103]\n",
      "100%|██████████| 156/156 [00:34<00:00,  4.49it/s, epoch=74/100, avg_loss=0.104]\n",
      "100%|██████████| 156/156 [00:35<00:00,  4.42it/s, epoch=75/100, avg_loss=0.102]\n",
      "100%|██████████| 156/156 [00:48<00:00,  3.23it/s, epoch=76/100, avg_loss=0.101]\n",
      "100%|██████████| 156/156 [00:57<00:00,  2.74it/s, epoch=77/100, avg_loss=0.103]\n",
      "100%|██████████| 156/156 [00:48<00:00,  3.23it/s, epoch=78/100, avg_loss=0.103]\n",
      "100%|██████████| 156/156 [00:38<00:00,  4.00it/s, epoch=79/100, avg_loss=0.102]\n",
      "100%|██████████| 156/156 [00:34<00:00,  4.52it/s, epoch=80/100, avg_loss=0.102]\n",
      "100%|██████████| 156/156 [00:35<00:00,  4.40it/s, epoch=81/100, avg_loss=0.1]   \n",
      "100%|██████████| 156/156 [00:59<00:00,  2.63it/s, epoch=82/100, avg_loss=0.0978]\n",
      "100%|██████████| 156/156 [00:42<00:00,  3.69it/s, epoch=83/100, avg_loss=0.0995]\n",
      "100%|██████████| 156/156 [00:43<00:00,  3.60it/s, epoch=84/100, avg_loss=0.101] \n",
      "100%|██████████| 156/156 [00:43<00:00,  3.62it/s, epoch=85/100, avg_loss=0.0986]\n",
      "100%|██████████| 156/156 [00:43<00:00,  3.61it/s, epoch=86/100, avg_loss=0.0997]\n",
      "100%|██████████| 156/156 [00:43<00:00,  3.61it/s, epoch=87/100, avg_loss=0.101]\n",
      "100%|██████████| 156/156 [00:49<00:00,  3.14it/s, epoch=88/100, avg_loss=0.0974]\n",
      "100%|██████████| 156/156 [00:53<00:00,  2.89it/s, epoch=89/100, avg_loss=0.0996]\n",
      "100%|██████████| 156/156 [00:52<00:00,  2.97it/s, epoch=90/100, avg_loss=0.0995]\n",
      "100%|██████████| 156/156 [00:48<00:00,  3.23it/s, epoch=91/100, avg_loss=0.0983]\n",
      "100%|██████████| 156/156 [00:43<00:00,  3.62it/s, epoch=92/100, avg_loss=0.0972]\n",
      "100%|██████████| 156/156 [00:37<00:00,  4.17it/s, epoch=93/100, avg_loss=0.0963]\n",
      "100%|██████████| 156/156 [00:33<00:00,  4.63it/s, epoch=94/100, avg_loss=0.0958]\n",
      "100%|██████████| 156/156 [00:35<00:00,  4.44it/s, epoch=95/100, avg_loss=0.0963]\n",
      "100%|██████████| 156/156 [00:33<00:00,  4.61it/s, epoch=96/100, avg_loss=0.0964]\n",
      "100%|██████████| 156/156 [00:34<00:00,  4.55it/s, epoch=97/100, avg_loss=0.0956]\n",
      "100%|██████████| 156/156 [00:34<00:00,  4.58it/s, epoch=98/100, avg_loss=0.0972]\n",
      "100%|██████████| 156/156 [00:34<00:00,  4.55it/s, epoch=99/100, avg_loss=0.0966]\n",
      "100%|██████████| 156/156 [00:40<00:00,  3.85it/s, epoch=100/100, avg_loss=0.0958]\n",
      "[I 2024-03-03 23:28:07,558] Trial 2 finished with value: 0.1637519763849059 and parameters: {'learning_rate': 1.8822711802390772e-05, 'batch_size': 64, 'num_layers': 16, 'residual_channels': 16, 'dilation_cycle_length': 4}. Best is trial 1 with value: 0.07706547251979637.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./model_checkpoints_optuna/model_epoch_100.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:25<00:00,  3.03it/s, epoch=1/100, avg_loss=1.05]\n",
      "100%|██████████| 78/78 [00:26<00:00,  2.91it/s, epoch=2/100, avg_loss=0.726]\n",
      "100%|██████████| 78/78 [00:26<00:00,  2.93it/s, epoch=3/100, avg_loss=0.529]\n",
      "100%|██████████| 78/78 [00:25<00:00,  3.09it/s, epoch=4/100, avg_loss=0.401]\n",
      "100%|██████████| 78/78 [00:24<00:00,  3.22it/s, epoch=5/100, avg_loss=0.309]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.36it/s, epoch=6/100, avg_loss=0.262]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.36it/s, epoch=7/100, avg_loss=0.252]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.30it/s, epoch=8/100, avg_loss=0.25] \n",
      "100%|██████████| 78/78 [00:23<00:00,  3.36it/s, epoch=9/100, avg_loss=0.234]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.34it/s, epoch=10/100, avg_loss=0.229]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.36it/s, epoch=11/100, avg_loss=0.233]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.34it/s, epoch=12/100, avg_loss=0.225]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.39it/s, epoch=13/100, avg_loss=0.22] \n",
      "100%|██████████| 78/78 [00:23<00:00,  3.35it/s, epoch=14/100, avg_loss=0.224]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.36it/s, epoch=15/100, avg_loss=0.215]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.36it/s, epoch=16/100, avg_loss=0.217]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.31it/s, epoch=17/100, avg_loss=0.223]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.30it/s, epoch=18/100, avg_loss=0.215]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.27it/s, epoch=19/100, avg_loss=0.206]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.35it/s, epoch=20/100, avg_loss=0.213]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.35it/s, epoch=21/100, avg_loss=0.209]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.34it/s, epoch=22/100, avg_loss=0.207]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.34it/s, epoch=23/100, avg_loss=0.21] \n",
      "100%|██████████| 78/78 [00:23<00:00,  3.37it/s, epoch=24/100, avg_loss=0.196]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.33it/s, epoch=25/100, avg_loss=0.201]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.38it/s, epoch=26/100, avg_loss=0.212]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.34it/s, epoch=27/100, avg_loss=0.195]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.32it/s, epoch=28/100, avg_loss=0.191]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.35it/s, epoch=29/100, avg_loss=0.192]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.31it/s, epoch=30/100, avg_loss=0.187]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.25it/s, epoch=31/100, avg_loss=0.189]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.32it/s, epoch=32/100, avg_loss=0.179]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.29it/s, epoch=33/100, avg_loss=0.183]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.30it/s, epoch=34/100, avg_loss=0.176]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.32it/s, epoch=35/100, avg_loss=0.178]\n",
      "100%|██████████| 78/78 [00:24<00:00,  3.24it/s, epoch=36/100, avg_loss=0.177]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.32it/s, epoch=37/100, avg_loss=0.178]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.27it/s, epoch=38/100, avg_loss=0.173]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.30it/s, epoch=39/100, avg_loss=0.178]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.32it/s, epoch=40/100, avg_loss=0.175]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.31it/s, epoch=41/100, avg_loss=0.177]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.31it/s, epoch=42/100, avg_loss=0.168]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.34it/s, epoch=43/100, avg_loss=0.164]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.35it/s, epoch=44/100, avg_loss=0.156]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.32it/s, epoch=45/100, avg_loss=0.165]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.35it/s, epoch=46/100, avg_loss=0.165]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.35it/s, epoch=47/100, avg_loss=0.159]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.27it/s, epoch=48/100, avg_loss=0.154]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.36it/s, epoch=49/100, avg_loss=0.155]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.32it/s, epoch=50/100, avg_loss=0.157]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.36it/s, epoch=51/100, avg_loss=0.155]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.32it/s, epoch=52/100, avg_loss=0.157]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.33it/s, epoch=53/100, avg_loss=0.147]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.35it/s, epoch=54/100, avg_loss=0.139]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.28it/s, epoch=55/100, avg_loss=0.146]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.28it/s, epoch=56/100, avg_loss=0.143]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.26it/s, epoch=57/100, avg_loss=0.141]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.35it/s, epoch=58/100, avg_loss=0.145]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.32it/s, epoch=59/100, avg_loss=0.144]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.32it/s, epoch=60/100, avg_loss=0.15] \n",
      "100%|██████████| 78/78 [00:23<00:00,  3.28it/s, epoch=61/100, avg_loss=0.143]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.31it/s, epoch=62/100, avg_loss=0.142]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.33it/s, epoch=63/100, avg_loss=0.145]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.31it/s, epoch=64/100, avg_loss=0.139]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.35it/s, epoch=65/100, avg_loss=0.136]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.30it/s, epoch=66/100, avg_loss=0.134]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.31it/s, epoch=67/100, avg_loss=0.137]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.27it/s, epoch=68/100, avg_loss=0.135]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.30it/s, epoch=69/100, avg_loss=0.132]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.33it/s, epoch=70/100, avg_loss=0.131]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.30it/s, epoch=71/100, avg_loss=0.131]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.29it/s, epoch=72/100, avg_loss=0.13] \n",
      "100%|██████████| 78/78 [00:23<00:00,  3.29it/s, epoch=73/100, avg_loss=0.132]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.28it/s, epoch=74/100, avg_loss=0.129]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.34it/s, epoch=75/100, avg_loss=0.128]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.30it/s, epoch=76/100, avg_loss=0.125]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.35it/s, epoch=77/100, avg_loss=0.13] \n",
      "100%|██████████| 78/78 [00:23<00:00,  3.35it/s, epoch=78/100, avg_loss=0.124]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.32it/s, epoch=79/100, avg_loss=0.124]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.33it/s, epoch=80/100, avg_loss=0.12] \n",
      "100%|██████████| 78/78 [00:23<00:00,  3.29it/s, epoch=81/100, avg_loss=0.125]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.28it/s, epoch=82/100, avg_loss=0.119]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.36it/s, epoch=83/100, avg_loss=0.124]\n",
      "100%|██████████| 78/78 [00:25<00:00,  3.03it/s, epoch=84/100, avg_loss=0.124]\n",
      "100%|██████████| 78/78 [00:27<00:00,  2.79it/s, epoch=85/100, avg_loss=0.12] \n",
      "100%|██████████| 78/78 [00:26<00:00,  2.93it/s, epoch=86/100, avg_loss=0.118]\n",
      "100%|██████████| 78/78 [00:27<00:00,  2.87it/s, epoch=87/100, avg_loss=0.123]\n",
      "100%|██████████| 78/78 [00:26<00:00,  2.90it/s, epoch=88/100, avg_loss=0.123]\n",
      "100%|██████████| 78/78 [00:25<00:00,  3.09it/s, epoch=89/100, avg_loss=0.121]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.33it/s, epoch=90/100, avg_loss=0.118]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.33it/s, epoch=91/100, avg_loss=0.114]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.26it/s, epoch=92/100, avg_loss=0.118]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.30it/s, epoch=93/100, avg_loss=0.118]\n",
      "100%|██████████| 78/78 [00:24<00:00,  3.24it/s, epoch=94/100, avg_loss=0.118]\n",
      "100%|██████████| 78/78 [00:24<00:00,  3.22it/s, epoch=95/100, avg_loss=0.117]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.34it/s, epoch=96/100, avg_loss=0.119]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.31it/s, epoch=97/100, avg_loss=0.111]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.36it/s, epoch=98/100, avg_loss=0.116]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.33it/s, epoch=99/100, avg_loss=0.114]\n",
      "100%|██████████| 78/78 [00:23<00:00,  3.33it/s, epoch=100/100, avg_loss=0.11] \n",
      "[I 2024-03-04 00:07:48,186] Trial 3 finished with value: 0.183238329917098 and parameters: {'learning_rate': 1.5082312898179672e-05, 'batch_size': 128, 'num_layers': 9, 'residual_channels': 32, 'dilation_cycle_length': 4}. Best is trial 1 with value: 0.07706547251979637.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./model_checkpoints_optuna/model_epoch_100.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [00:35<00:00,  4.45it/s, epoch=1/100, avg_loss=0.788]\n",
      "100%|██████████| 156/156 [00:37<00:00,  4.20it/s, epoch=2/100, avg_loss=0.298]\n",
      "100%|██████████| 156/156 [00:37<00:00,  4.16it/s, epoch=3/100, avg_loss=0.232]\n",
      "100%|██████████| 156/156 [00:36<00:00,  4.24it/s, epoch=4/100, avg_loss=0.217]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.95it/s, epoch=5/100, avg_loss=0.204]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.90it/s, epoch=6/100, avg_loss=0.187]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.93it/s, epoch=7/100, avg_loss=0.175]\n",
      "100%|██████████| 156/156 [00:32<00:00,  4.83it/s, epoch=8/100, avg_loss=0.164]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.95it/s, epoch=9/100, avg_loss=0.157]\n",
      "100%|██████████| 156/156 [00:32<00:00,  4.87it/s, epoch=10/100, avg_loss=0.146]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.90it/s, epoch=11/100, avg_loss=0.14] \n",
      "100%|██████████| 156/156 [00:32<00:00,  4.86it/s, epoch=12/100, avg_loss=0.136]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.89it/s, epoch=13/100, avg_loss=0.123]\n",
      "100%|██████████| 156/156 [00:38<00:00,  4.10it/s, epoch=14/100, avg_loss=0.123]\n",
      "100%|██████████| 156/156 [00:36<00:00,  4.33it/s, epoch=15/100, avg_loss=0.121]\n",
      "100%|██████████| 156/156 [00:37<00:00,  4.17it/s, epoch=16/100, avg_loss=0.116]\n",
      "100%|██████████| 156/156 [00:33<00:00,  4.69it/s, epoch=17/100, avg_loss=0.114]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.94it/s, epoch=18/100, avg_loss=0.112]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.97it/s, epoch=19/100, avg_loss=0.11] \n",
      "100%|██████████| 156/156 [00:31<00:00,  4.93it/s, epoch=20/100, avg_loss=0.105]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.98it/s, epoch=21/100, avg_loss=0.105]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.88it/s, epoch=22/100, avg_loss=0.103]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.93it/s, epoch=23/100, avg_loss=0.103]\n",
      "100%|██████████| 156/156 [00:32<00:00,  4.84it/s, epoch=24/100, avg_loss=0.101]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.97it/s, epoch=25/100, avg_loss=0.0996]\n",
      "100%|██████████| 156/156 [00:34<00:00,  4.58it/s, epoch=26/100, avg_loss=0.0985]\n",
      "100%|██████████| 156/156 [00:37<00:00,  4.18it/s, epoch=27/100, avg_loss=0.0992]\n",
      "100%|██████████| 156/156 [00:36<00:00,  4.24it/s, epoch=28/100, avg_loss=0.0971]\n",
      "100%|██████████| 156/156 [00:36<00:00,  4.23it/s, epoch=29/100, avg_loss=0.0956]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.89it/s, epoch=30/100, avg_loss=0.0971]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.94it/s, epoch=31/100, avg_loss=0.0955]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.88it/s, epoch=32/100, avg_loss=0.0952]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.94it/s, epoch=33/100, avg_loss=0.0922]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.88it/s, epoch=34/100, avg_loss=0.095] \n",
      "100%|██████████| 156/156 [00:32<00:00,  4.87it/s, epoch=35/100, avg_loss=0.0943]\n",
      "100%|██████████| 156/156 [00:34<00:00,  4.53it/s, epoch=36/100, avg_loss=0.0917]\n",
      "100%|██████████| 156/156 [00:36<00:00,  4.25it/s, epoch=37/100, avg_loss=0.0924]\n",
      "100%|██████████| 156/156 [00:36<00:00,  4.24it/s, epoch=38/100, avg_loss=0.0927]\n",
      "100%|██████████| 156/156 [00:36<00:00,  4.25it/s, epoch=39/100, avg_loss=0.0923]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.93it/s, epoch=40/100, avg_loss=0.0888]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.98it/s, epoch=41/100, avg_loss=0.0911]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.95it/s, epoch=42/100, avg_loss=0.0885]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.98it/s, epoch=43/100, avg_loss=0.0908]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.89it/s, epoch=44/100, avg_loss=0.0885]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.98it/s, epoch=45/100, avg_loss=0.0888]\n",
      "100%|██████████| 156/156 [00:33<00:00,  4.63it/s, epoch=46/100, avg_loss=0.0882]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.96it/s, epoch=47/100, avg_loss=0.0865]\n",
      "100%|██████████| 156/156 [00:37<00:00,  4.17it/s, epoch=48/100, avg_loss=0.0873]\n",
      "100%|██████████| 156/156 [00:37<00:00,  4.21it/s, epoch=49/100, avg_loss=0.0861]\n",
      "100%|██████████| 156/156 [00:38<00:00,  4.09it/s, epoch=50/100, avg_loss=0.0876]\n",
      "100%|██████████| 156/156 [00:34<00:00,  4.50it/s, epoch=51/100, avg_loss=0.0866]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.89it/s, epoch=52/100, avg_loss=0.088] \n",
      "100%|██████████| 156/156 [00:31<00:00,  4.93it/s, epoch=53/100, avg_loss=0.0859]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.97it/s, epoch=54/100, avg_loss=0.0862]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.96it/s, epoch=55/100, avg_loss=0.086] \n",
      "100%|██████████| 156/156 [00:31<00:00,  4.95it/s, epoch=56/100, avg_loss=0.0857]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.94it/s, epoch=57/100, avg_loss=0.086] \n",
      "100%|██████████| 156/156 [00:31<00:00,  4.90it/s, epoch=58/100, avg_loss=0.0848]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.88it/s, epoch=59/100, avg_loss=0.0865]\n",
      "100%|██████████| 156/156 [00:33<00:00,  4.66it/s, epoch=60/100, avg_loss=0.0837]\n",
      "100%|██████████| 156/156 [00:37<00:00,  4.12it/s, epoch=61/100, avg_loss=0.0838]\n",
      "100%|██████████| 156/156 [00:36<00:00,  4.25it/s, epoch=62/100, avg_loss=0.0845]\n",
      "100%|██████████| 156/156 [00:36<00:00,  4.24it/s, epoch=63/100, avg_loss=0.0851]\n",
      "100%|██████████| 156/156 [00:32<00:00,  4.76it/s, epoch=64/100, avg_loss=0.0844]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.94it/s, epoch=65/100, avg_loss=0.0862]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.98it/s, epoch=66/100, avg_loss=0.0848]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.95it/s, epoch=67/100, avg_loss=0.0853]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.95it/s, epoch=68/100, avg_loss=0.0844]\n",
      "100%|██████████| 156/156 [00:32<00:00,  4.85it/s, epoch=69/100, avg_loss=0.0835]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.96it/s, epoch=70/100, avg_loss=0.0816]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.91it/s, epoch=71/100, avg_loss=0.0844]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.96it/s, epoch=72/100, avg_loss=0.0831]\n",
      "100%|██████████| 156/156 [00:34<00:00,  4.48it/s, epoch=73/100, avg_loss=0.0842]\n",
      "100%|██████████| 156/156 [00:36<00:00,  4.23it/s, epoch=74/100, avg_loss=0.0811]\n",
      "100%|██████████| 156/156 [00:37<00:00,  4.19it/s, epoch=75/100, avg_loss=0.0828]\n",
      "100%|██████████| 156/156 [00:36<00:00,  4.32it/s, epoch=76/100, avg_loss=0.0839]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.97it/s, epoch=77/100, avg_loss=0.0815]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.88it/s, epoch=78/100, avg_loss=0.0829]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.91it/s, epoch=79/100, avg_loss=0.0832]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.88it/s, epoch=80/100, avg_loss=0.0826]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.94it/s, epoch=81/100, avg_loss=0.0823]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.98it/s, epoch=82/100, avg_loss=0.0831]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.98it/s, epoch=83/100, avg_loss=0.0822]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.97it/s, epoch=84/100, avg_loss=0.082] \n",
      "100%|██████████| 156/156 [00:31<00:00,  4.92it/s, epoch=85/100, avg_loss=0.0822]\n",
      "100%|██████████| 156/156 [00:37<00:00,  4.20it/s, epoch=86/100, avg_loss=0.0791]\n",
      "100%|██████████| 156/156 [00:36<00:00,  4.25it/s, epoch=87/100, avg_loss=0.0808]\n",
      "100%|██████████| 156/156 [00:37<00:00,  4.19it/s, epoch=88/100, avg_loss=0.0817]\n",
      "100%|██████████| 156/156 [00:35<00:00,  4.34it/s, epoch=89/100, avg_loss=0.081] \n",
      "100%|██████████| 156/156 [00:31<00:00,  4.93it/s, epoch=90/100, avg_loss=0.0811]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.94it/s, epoch=91/100, avg_loss=0.0802]\n",
      "100%|██████████| 156/156 [00:31<00:00,  5.00it/s, epoch=92/100, avg_loss=0.0824]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.96it/s, epoch=93/100, avg_loss=0.0813]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.93it/s, epoch=94/100, avg_loss=0.0803]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.92it/s, epoch=95/100, avg_loss=0.0815]\n",
      "100%|██████████| 156/156 [00:31<00:00,  4.98it/s, epoch=96/100, avg_loss=0.0806]\n",
      "100%|██████████| 156/156 [00:31<00:00,  5.01it/s, epoch=97/100, avg_loss=0.0797]\n",
      "100%|██████████| 156/156 [00:32<00:00,  4.78it/s, epoch=98/100, avg_loss=0.08]  \n",
      "100%|██████████| 156/156 [00:37<00:00,  4.14it/s, epoch=99/100, avg_loss=0.0805]\n",
      "100%|██████████| 156/156 [00:36<00:00,  4.32it/s, epoch=100/100, avg_loss=0.0799]\n",
      "[I 2024-03-04 01:03:20,014] Trial 4 finished with value: 0.10736222193428097 and parameters: {'learning_rate': 8.255788603673755e-05, 'batch_size': 64, 'num_layers': 15, 'residual_channels': 16, 'dilation_cycle_length': 2}. Best is trial 1 with value: 0.07706547251979637.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./model_checkpoints_optuna/model_epoch_100.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:13<00:00,  5.93it/s, epoch=1/100, avg_loss=0.775]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.92it/s, epoch=2/100, avg_loss=0.305]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.87it/s, epoch=3/100, avg_loss=0.233]\n",
      "100%|██████████| 78/78 [00:12<00:00,  6.24it/s, epoch=4/100, avg_loss=0.228]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.87it/s, epoch=5/100, avg_loss=0.22] \n",
      "100%|██████████| 78/78 [00:11<00:00,  6.85it/s, epoch=6/100, avg_loss=0.207]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.73it/s, epoch=7/100, avg_loss=0.205]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.74it/s, epoch=8/100, avg_loss=0.204]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.79it/s, epoch=9/100, avg_loss=0.205]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.82it/s, epoch=10/100, avg_loss=0.203]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.79it/s, epoch=11/100, avg_loss=0.2]  \n",
      "100%|██████████| 78/78 [00:11<00:00,  6.82it/s, epoch=12/100, avg_loss=0.185]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.64it/s, epoch=13/100, avg_loss=0.18] \n",
      "100%|██████████| 78/78 [00:11<00:00,  6.50it/s, epoch=14/100, avg_loss=0.183]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.59it/s, epoch=15/100, avg_loss=0.168]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.85it/s, epoch=16/100, avg_loss=0.165]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.77it/s, epoch=17/100, avg_loss=0.162]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.71it/s, epoch=18/100, avg_loss=0.159]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.53it/s, epoch=19/100, avg_loss=0.16] \n",
      "100%|██████████| 78/78 [00:11<00:00,  6.73it/s, epoch=20/100, avg_loss=0.16] \n",
      "100%|██████████| 78/78 [00:11<00:00,  6.53it/s, epoch=21/100, avg_loss=0.147]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.76it/s, epoch=22/100, avg_loss=0.153]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.74it/s, epoch=23/100, avg_loss=0.148]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.64it/s, epoch=24/100, avg_loss=0.143]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.68it/s, epoch=25/100, avg_loss=0.139]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.69it/s, epoch=26/100, avg_loss=0.134]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.75it/s, epoch=27/100, avg_loss=0.14] \n",
      "100%|██████████| 78/78 [00:11<00:00,  6.76it/s, epoch=28/100, avg_loss=0.14] \n",
      "100%|██████████| 78/78 [00:12<00:00,  6.39it/s, epoch=29/100, avg_loss=0.132]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.76it/s, epoch=30/100, avg_loss=0.132]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.78it/s, epoch=31/100, avg_loss=0.128]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.62it/s, epoch=32/100, avg_loss=0.129]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.90it/s, epoch=33/100, avg_loss=0.124]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.78it/s, epoch=34/100, avg_loss=0.124]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.71it/s, epoch=35/100, avg_loss=0.125]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.85it/s, epoch=36/100, avg_loss=0.12] \n",
      "100%|██████████| 78/78 [00:13<00:00,  5.88it/s, epoch=37/100, avg_loss=0.118]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.85it/s, epoch=38/100, avg_loss=0.121]\n",
      "100%|██████████| 78/78 [00:12<00:00,  6.13it/s, epoch=39/100, avg_loss=0.119]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.58it/s, epoch=40/100, avg_loss=0.114]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.78it/s, epoch=41/100, avg_loss=0.116]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.78it/s, epoch=42/100, avg_loss=0.112]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.72it/s, epoch=43/100, avg_loss=0.108]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.55it/s, epoch=44/100, avg_loss=0.112]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.79it/s, epoch=45/100, avg_loss=0.112]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.67it/s, epoch=46/100, avg_loss=0.111]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.85it/s, epoch=47/100, avg_loss=0.111]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.81it/s, epoch=48/100, avg_loss=0.109]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.89it/s, epoch=49/100, avg_loss=0.112]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.85it/s, epoch=50/100, avg_loss=0.108]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.77it/s, epoch=51/100, avg_loss=0.11] \n",
      "100%|██████████| 78/78 [00:11<00:00,  6.85it/s, epoch=52/100, avg_loss=0.107]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.85it/s, epoch=53/100, avg_loss=0.111]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.81it/s, epoch=54/100, avg_loss=0.111]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.77it/s, epoch=55/100, avg_loss=0.106]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.70it/s, epoch=56/100, avg_loss=0.106]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.84it/s, epoch=57/100, avg_loss=0.106]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.76it/s, epoch=58/100, avg_loss=0.103] \n",
      "100%|██████████| 78/78 [00:11<00:00,  6.77it/s, epoch=59/100, avg_loss=0.103] \n",
      "100%|██████████| 78/78 [00:11<00:00,  6.81it/s, epoch=60/100, avg_loss=0.103]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.84it/s, epoch=61/100, avg_loss=0.102] \n",
      "100%|██████████| 78/78 [00:11<00:00,  6.88it/s, epoch=62/100, avg_loss=0.104] \n",
      "100%|██████████| 78/78 [00:11<00:00,  6.73it/s, epoch=63/100, avg_loss=0.104] \n",
      "100%|██████████| 78/78 [00:12<00:00,  6.23it/s, epoch=64/100, avg_loss=0.104]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.93it/s, epoch=65/100, avg_loss=0.101] \n",
      "100%|██████████| 78/78 [00:14<00:00,  5.53it/s, epoch=66/100, avg_loss=0.1]   \n",
      "100%|██████████| 78/78 [00:13<00:00,  5.87it/s, epoch=67/100, avg_loss=0.105] \n",
      "100%|██████████| 78/78 [00:13<00:00,  5.88it/s, epoch=68/100, avg_loss=0.1]   \n",
      "100%|██████████| 78/78 [00:13<00:00,  5.76it/s, epoch=69/100, avg_loss=0.101] \n",
      "100%|██████████| 78/78 [00:12<00:00,  6.01it/s, epoch=70/100, avg_loss=0.103]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.98it/s, epoch=71/100, avg_loss=0.0997]\n",
      "100%|██████████| 78/78 [00:12<00:00,  6.07it/s, epoch=72/100, avg_loss=0.103]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.85it/s, epoch=73/100, avg_loss=0.1]   \n",
      "100%|██████████| 78/78 [00:12<00:00,  6.34it/s, epoch=74/100, avg_loss=0.101] \n",
      "100%|██████████| 78/78 [00:11<00:00,  6.78it/s, epoch=75/100, avg_loss=0.101] \n",
      "100%|██████████| 78/78 [00:11<00:00,  6.76it/s, epoch=76/100, avg_loss=0.1]   \n",
      "100%|██████████| 78/78 [00:11<00:00,  6.70it/s, epoch=77/100, avg_loss=0.101] \n",
      "100%|██████████| 78/78 [00:11<00:00,  6.76it/s, epoch=78/100, avg_loss=0.101] \n",
      "100%|██████████| 78/78 [00:11<00:00,  6.70it/s, epoch=79/100, avg_loss=0.1]   \n",
      "100%|██████████| 78/78 [00:11<00:00,  6.81it/s, epoch=80/100, avg_loss=0.103]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.71it/s, epoch=81/100, avg_loss=0.0971]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.80it/s, epoch=82/100, avg_loss=0.0973]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.71it/s, epoch=83/100, avg_loss=0.0995]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.82it/s, epoch=84/100, avg_loss=0.0994]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.61it/s, epoch=85/100, avg_loss=0.0967]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.88it/s, epoch=86/100, avg_loss=0.0976]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.89it/s, epoch=87/100, avg_loss=0.098] \n",
      "100%|██████████| 78/78 [00:11<00:00,  6.82it/s, epoch=88/100, avg_loss=0.0998]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.54it/s, epoch=89/100, avg_loss=0.0968]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.79it/s, epoch=90/100, avg_loss=0.097] \n",
      "100%|██████████| 78/78 [00:11<00:00,  6.84it/s, epoch=91/100, avg_loss=0.0988]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.84it/s, epoch=92/100, avg_loss=0.101] \n",
      "100%|██████████| 78/78 [00:11<00:00,  6.60it/s, epoch=93/100, avg_loss=0.0973]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.65it/s, epoch=94/100, avg_loss=0.0988]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.89it/s, epoch=95/100, avg_loss=0.0974]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.72it/s, epoch=96/100, avg_loss=0.0932]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.92it/s, epoch=97/100, avg_loss=0.0943]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.76it/s, epoch=98/100, avg_loss=0.0949]\n",
      "100%|██████████| 78/78 [00:12<00:00,  6.14it/s, epoch=99/100, avg_loss=0.0964]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.90it/s, epoch=100/100, avg_loss=0.0957]\n",
      "[I 2024-03-04 01:23:21,140] Trial 5 finished with value: 0.13291951138714825 and parameters: {'learning_rate': 4.978849500981284e-05, 'batch_size': 128, 'num_layers': 4, 'residual_channels': 32, 'dilation_cycle_length': 4}. Best is trial 1 with value: 0.07706547251979637.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./model_checkpoints_optuna/model_epoch_100.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [02:03<00:00,  2.53it/s, epoch=1/100, avg_loss=0.279]\n",
      "100%|██████████| 312/312 [01:49<00:00,  2.84it/s, epoch=2/100, avg_loss=0.168]\n",
      "100%|██████████| 312/312 [02:04<00:00,  2.52it/s, epoch=3/100, avg_loss=0.135]\n",
      "100%|██████████| 312/312 [02:05<00:00,  2.48it/s, epoch=4/100, avg_loss=0.115]\n",
      "100%|██████████| 312/312 [01:55<00:00,  2.69it/s, epoch=5/100, avg_loss=0.104]\n",
      "100%|██████████| 312/312 [01:48<00:00,  2.88it/s, epoch=6/100, avg_loss=0.0999]\n",
      "100%|██████████| 312/312 [01:48<00:00,  2.87it/s, epoch=7/100, avg_loss=0.095] \n",
      "100%|██████████| 312/312 [02:05<00:00,  2.50it/s, epoch=8/100, avg_loss=0.0907]\n",
      "100%|██████████| 312/312 [01:54<00:00,  2.72it/s, epoch=9/100, avg_loss=0.0888]\n",
      "100%|██████████| 312/312 [01:48<00:00,  2.86it/s, epoch=10/100, avg_loss=0.0877]\n",
      "100%|██████████| 312/312 [01:54<00:00,  2.73it/s, epoch=11/100, avg_loss=0.0872]\n",
      "100%|██████████| 312/312 [01:59<00:00,  2.61it/s, epoch=12/100, avg_loss=0.0878]\n",
      "100%|██████████| 312/312 [01:48<00:00,  2.88it/s, epoch=13/100, avg_loss=0.0855]\n",
      "100%|██████████| 312/312 [01:48<00:00,  2.89it/s, epoch=14/100, avg_loss=0.0845]\n",
      "100%|██████████| 312/312 [02:00<00:00,  2.60it/s, epoch=15/100, avg_loss=0.0843]\n",
      "100%|██████████| 312/312 [01:55<00:00,  2.69it/s, epoch=16/100, avg_loss=0.0837]\n",
      "100%|██████████| 312/312 [01:48<00:00,  2.87it/s, epoch=17/100, avg_loss=0.0821]\n",
      "100%|██████████| 312/312 [01:47<00:00,  2.89it/s, epoch=18/100, avg_loss=0.0808]\n",
      "100%|██████████| 312/312 [02:01<00:00,  2.56it/s, epoch=19/100, avg_loss=0.0812]\n",
      "100%|██████████| 312/312 [01:50<00:00,  2.83it/s, epoch=20/100, avg_loss=0.0803]\n",
      "100%|██████████| 312/312 [01:47<00:00,  2.91it/s, epoch=21/100, avg_loss=0.08]  \n",
      "100%|██████████| 312/312 [01:46<00:00,  2.93it/s, epoch=22/100, avg_loss=0.0798]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.92it/s, epoch=23/100, avg_loss=0.0797]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.93it/s, epoch=24/100, avg_loss=0.0801]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.92it/s, epoch=25/100, avg_loss=0.0787]\n",
      "100%|██████████| 312/312 [01:48<00:00,  2.88it/s, epoch=26/100, avg_loss=0.0781]\n",
      "100%|██████████| 312/312 [01:47<00:00,  2.91it/s, epoch=27/100, avg_loss=0.0779]\n",
      "100%|██████████| 312/312 [01:47<00:00,  2.91it/s, epoch=28/100, avg_loss=0.0776]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.92it/s, epoch=29/100, avg_loss=0.077] \n",
      "100%|██████████| 312/312 [01:46<00:00,  2.93it/s, epoch=30/100, avg_loss=0.0769]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.93it/s, epoch=31/100, avg_loss=0.0773]\n",
      "100%|██████████| 312/312 [01:47<00:00,  2.91it/s, epoch=32/100, avg_loss=0.0755]\n",
      "100%|██████████| 312/312 [01:47<00:00,  2.91it/s, epoch=33/100, avg_loss=0.0763]\n",
      "100%|██████████| 312/312 [01:47<00:00,  2.91it/s, epoch=34/100, avg_loss=0.0758]\n",
      "100%|██████████| 312/312 [01:55<00:00,  2.71it/s, epoch=35/100, avg_loss=0.0755]\n",
      "100%|██████████| 312/312 [01:57<00:00,  2.66it/s, epoch=36/100, avg_loss=0.0749]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.93it/s, epoch=37/100, avg_loss=0.0758]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.92it/s, epoch=38/100, avg_loss=0.0751]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.92it/s, epoch=39/100, avg_loss=0.0751]\n",
      "100%|██████████| 312/312 [01:47<00:00,  2.89it/s, epoch=40/100, avg_loss=0.0743]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.94it/s, epoch=41/100, avg_loss=0.0743]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.92it/s, epoch=42/100, avg_loss=0.0736]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.93it/s, epoch=43/100, avg_loss=0.0741]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.93it/s, epoch=44/100, avg_loss=0.0735]\n",
      "100%|██████████| 312/312 [01:47<00:00,  2.91it/s, epoch=45/100, avg_loss=0.0733]\n",
      "100%|██████████| 312/312 [01:45<00:00,  2.94it/s, epoch=46/100, avg_loss=0.0735]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.93it/s, epoch=47/100, avg_loss=0.0731]\n",
      "100%|██████████| 312/312 [01:47<00:00,  2.90it/s, epoch=48/100, avg_loss=0.0718]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.94it/s, epoch=49/100, avg_loss=0.0721]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.92it/s, epoch=50/100, avg_loss=0.0735]\n",
      "100%|██████████| 312/312 [01:48<00:00,  2.88it/s, epoch=51/100, avg_loss=0.0722]\n",
      "100%|██████████| 312/312 [01:47<00:00,  2.90it/s, epoch=52/100, avg_loss=0.0726]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.92it/s, epoch=53/100, avg_loss=0.0724]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.93it/s, epoch=54/100, avg_loss=0.0718]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.94it/s, epoch=55/100, avg_loss=0.0711]\n",
      "100%|██████████| 312/312 [01:45<00:00,  2.94it/s, epoch=56/100, avg_loss=0.0721]\n",
      "100%|██████████| 312/312 [01:47<00:00,  2.89it/s, epoch=57/100, avg_loss=0.0715]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.94it/s, epoch=58/100, avg_loss=0.071] \n",
      "100%|██████████| 312/312 [01:46<00:00,  2.94it/s, epoch=59/100, avg_loss=0.0716]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.93it/s, epoch=60/100, avg_loss=0.0711]\n",
      "100%|██████████| 312/312 [01:50<00:00,  2.83it/s, epoch=61/100, avg_loss=0.0715]\n",
      "100%|██████████| 312/312 [01:45<00:00,  2.95it/s, epoch=62/100, avg_loss=0.0713]\n",
      "100%|██████████| 312/312 [01:47<00:00,  2.91it/s, epoch=63/100, avg_loss=0.0708]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.92it/s, epoch=64/100, avg_loss=0.0714]\n",
      "100%|██████████| 312/312 [01:47<00:00,  2.90it/s, epoch=65/100, avg_loss=0.0711]\n",
      "100%|██████████| 312/312 [01:45<00:00,  2.95it/s, epoch=66/100, avg_loss=0.0709]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.93it/s, epoch=67/100, avg_loss=0.0707]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.93it/s, epoch=68/100, avg_loss=0.0705]\n",
      "100%|██████████| 312/312 [02:03<00:00,  2.53it/s, epoch=69/100, avg_loss=0.0703]\n",
      "100%|██████████| 312/312 [01:58<00:00,  2.64it/s, epoch=70/100, avg_loss=0.0706]\n",
      "100%|██████████| 312/312 [02:06<00:00,  2.47it/s, epoch=71/100, avg_loss=0.0695]\n",
      "100%|██████████| 312/312 [01:45<00:00,  2.94it/s, epoch=72/100, avg_loss=0.0711]\n",
      "100%|██████████| 312/312 [01:47<00:00,  2.91it/s, epoch=73/100, avg_loss=0.0706]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.94it/s, epoch=74/100, avg_loss=0.0702]\n",
      "100%|██████████| 312/312 [01:47<00:00,  2.90it/s, epoch=75/100, avg_loss=0.0694]\n",
      "100%|██████████| 312/312 [01:47<00:00,  2.91it/s, epoch=76/100, avg_loss=0.07]  \n",
      "100%|██████████| 312/312 [01:46<00:00,  2.94it/s, epoch=77/100, avg_loss=0.069] \n",
      "100%|██████████| 312/312 [01:46<00:00,  2.94it/s, epoch=78/100, avg_loss=0.0693]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.94it/s, epoch=79/100, avg_loss=0.0697]\n",
      "100%|██████████| 312/312 [01:45<00:00,  2.95it/s, epoch=80/100, avg_loss=0.069] \n",
      "100%|██████████| 312/312 [01:46<00:00,  2.92it/s, epoch=81/100, avg_loss=0.0692]\n",
      "100%|██████████| 312/312 [01:47<00:00,  2.91it/s, epoch=82/100, avg_loss=0.0694]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.94it/s, epoch=83/100, avg_loss=0.0687]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.92it/s, epoch=84/100, avg_loss=0.0694]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.92it/s, epoch=85/100, avg_loss=0.0707]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.94it/s, epoch=86/100, avg_loss=0.0689]\n",
      "100%|██████████| 312/312 [01:47<00:00,  2.91it/s, epoch=87/100, avg_loss=0.0694]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.92it/s, epoch=88/100, avg_loss=0.0689]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.93it/s, epoch=89/100, avg_loss=0.0684]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.92it/s, epoch=90/100, avg_loss=0.0678]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.94it/s, epoch=91/100, avg_loss=0.0702]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.93it/s, epoch=92/100, avg_loss=0.0691]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.93it/s, epoch=93/100, avg_loss=0.069] \n",
      "100%|██████████| 312/312 [01:46<00:00,  2.93it/s, epoch=94/100, avg_loss=0.0681]\n",
      "100%|██████████| 312/312 [01:45<00:00,  2.95it/s, epoch=95/100, avg_loss=0.0685]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.94it/s, epoch=96/100, avg_loss=0.0689]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.94it/s, epoch=97/100, avg_loss=0.0685]\n",
      "100%|██████████| 312/312 [01:46<00:00,  2.94it/s, epoch=98/100, avg_loss=0.0678]\n",
      "100%|██████████| 312/312 [01:47<00:00,  2.90it/s, epoch=99/100, avg_loss=0.0683]\n",
      "100%|██████████| 312/312 [37:06<00:00,  7.14s/it, epoch=100/100, avg_loss=0.0683]   \n",
      "[I 2024-03-04 05:00:26,171] Trial 6 finished with value: 0.0787650188115744 and parameters: {'learning_rate': 0.00011918208217766387, 'batch_size': 32, 'num_layers': 11, 'residual_channels': 32, 'dilation_cycle_length': 1}. Best is trial 1 with value: 0.07706547251979637.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./model_checkpoints_optuna/model_epoch_100.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:14<00:00,  5.24it/s, epoch=1/100, avg_loss=0.837]\n",
      "100%|██████████| 78/78 [2:47:33<00:00, 128.90s/it, epoch=2/100, avg_loss=0.555]    \n",
      "100%|██████████| 78/78 [00:12<00:00,  6.20it/s, epoch=3/100, avg_loss=0.381]\n",
      "100%|██████████| 78/78 [00:12<00:00,  6.16it/s, epoch=4/100, avg_loss=0.293]\n",
      "100%|██████████| 78/78 [00:12<00:00,  6.12it/s, epoch=5/100, avg_loss=0.261]\n",
      "100%|██████████| 78/78 [00:11<00:00,  6.55it/s, epoch=6/100, avg_loss=0.243]\n",
      "100%|██████████| 78/78 [00:12<00:00,  6.01it/s, epoch=7/100, avg_loss=0.239]\n",
      "100%|██████████| 78/78 [00:12<00:00,  6.28it/s, epoch=8/100, avg_loss=0.234]\n",
      "100%|██████████| 78/78 [00:12<00:00,  6.42it/s, epoch=9/100, avg_loss=0.224]\n",
      "100%|██████████| 78/78 [00:12<00:00,  6.41it/s, epoch=10/100, avg_loss=0.224]\n",
      "100%|██████████| 78/78 [1:11:44<00:00, 55.19s/it, epoch=11/100, avg_loss=0.226]     \n",
      "100%|██████████| 78/78 [00:12<00:00,  6.33it/s, epoch=12/100, avg_loss=0.212]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.78it/s, epoch=13/100, avg_loss=0.209]\n",
      "100%|██████████| 78/78 [00:12<00:00,  6.14it/s, epoch=14/100, avg_loss=0.211]\n",
      "100%|██████████| 78/78 [00:12<00:00,  6.30it/s, epoch=15/100, avg_loss=0.212]\n",
      "100%|██████████| 78/78 [00:12<00:00,  6.26it/s, epoch=16/100, avg_loss=0.202]\n",
      "100%|██████████| 78/78 [00:16<00:00,  4.67it/s, epoch=17/100, avg_loss=0.198]\n",
      "100%|██████████| 78/78 [00:15<00:00,  5.08it/s, epoch=18/100, avg_loss=0.198]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.53it/s, epoch=19/100, avg_loss=0.2]  \n",
      "100%|██████████| 78/78 [00:14<00:00,  5.46it/s, epoch=20/100, avg_loss=0.189]\n",
      "100%|██████████| 78/78 [00:12<00:00,  6.18it/s, epoch=21/100, avg_loss=0.191]\n",
      "100%|██████████| 78/78 [00:12<00:00,  6.23it/s, epoch=22/100, avg_loss=0.188]\n",
      "100%|██████████| 78/78 [00:12<00:00,  6.29it/s, epoch=23/100, avg_loss=0.188]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.99it/s, epoch=24/100, avg_loss=0.187]\n",
      "100%|██████████| 78/78 [00:12<00:00,  6.17it/s, epoch=25/100, avg_loss=0.181]\n",
      "100%|██████████| 78/78 [00:12<00:00,  6.11it/s, epoch=26/100, avg_loss=0.166]\n",
      "100%|██████████| 78/78 [00:12<00:00,  6.21it/s, epoch=27/100, avg_loss=0.172]\n",
      "100%|██████████| 78/78 [00:12<00:00,  6.22it/s, epoch=28/100, avg_loss=0.174]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.79it/s, epoch=29/100, avg_loss=0.167]\n",
      "100%|██████████| 78/78 [00:12<00:00,  6.03it/s, epoch=30/100, avg_loss=0.166]\n",
      "100%|██████████| 78/78 [00:12<00:00,  6.02it/s, epoch=31/100, avg_loss=0.163]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.81it/s, epoch=32/100, avg_loss=0.161]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.90it/s, epoch=33/100, avg_loss=0.167]\n",
      "100%|██████████| 78/78 [00:12<00:00,  6.01it/s, epoch=34/100, avg_loss=0.158]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.47it/s, epoch=35/100, avg_loss=0.153]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.77it/s, epoch=36/100, avg_loss=0.154]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.84it/s, epoch=37/100, avg_loss=0.151]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.82it/s, epoch=38/100, avg_loss=0.149]\n",
      "100%|██████████| 78/78 [00:15<00:00,  5.05it/s, epoch=39/100, avg_loss=0.15] \n",
      "100%|██████████| 78/78 [00:16<00:00,  4.87it/s, epoch=40/100, avg_loss=0.152]\n",
      "100%|██████████| 78/78 [00:16<00:00,  4.62it/s, epoch=41/100, avg_loss=0.151]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.28it/s, epoch=42/100, avg_loss=0.154]\n",
      "100%|██████████| 78/78 [00:16<00:00,  4.61it/s, epoch=43/100, avg_loss=0.146]\n",
      "100%|██████████| 78/78 [00:15<00:00,  4.97it/s, epoch=44/100, avg_loss=0.146]\n",
      "100%|██████████| 78/78 [00:16<00:00,  4.81it/s, epoch=45/100, avg_loss=0.143]\n",
      "100%|██████████| 78/78 [00:15<00:00,  5.00it/s, epoch=46/100, avg_loss=0.148]\n",
      "100%|██████████| 78/78 [00:16<00:00,  4.76it/s, epoch=47/100, avg_loss=0.145]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.52it/s, epoch=48/100, avg_loss=0.142]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.60it/s, epoch=49/100, avg_loss=0.139]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.63it/s, epoch=50/100, avg_loss=0.139]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.59it/s, epoch=51/100, avg_loss=0.136]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.58it/s, epoch=52/100, avg_loss=0.137]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.50it/s, epoch=53/100, avg_loss=0.133]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.39it/s, epoch=54/100, avg_loss=0.137]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.61it/s, epoch=55/100, avg_loss=0.136]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.36it/s, epoch=56/100, avg_loss=0.131]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.39it/s, epoch=57/100, avg_loss=0.131]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.35it/s, epoch=58/100, avg_loss=0.127]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.63it/s, epoch=59/100, avg_loss=0.13] \n",
      "100%|██████████| 78/78 [00:14<00:00,  5.45it/s, epoch=60/100, avg_loss=0.126]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.70it/s, epoch=61/100, avg_loss=0.128]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.46it/s, epoch=62/100, avg_loss=0.125]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.59it/s, epoch=63/100, avg_loss=0.129]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.51it/s, epoch=64/100, avg_loss=0.122]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.62it/s, epoch=65/100, avg_loss=0.129]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.41it/s, epoch=66/100, avg_loss=0.123]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.24it/s, epoch=67/100, avg_loss=0.122]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.66it/s, epoch=68/100, avg_loss=0.123]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.47it/s, epoch=69/100, avg_loss=0.127]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.58it/s, epoch=70/100, avg_loss=0.117]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.58it/s, epoch=71/100, avg_loss=0.118]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.38it/s, epoch=72/100, avg_loss=0.118]\n",
      "100%|██████████| 78/78 [00:15<00:00,  4.90it/s, epoch=73/100, avg_loss=0.114]\n",
      "100%|██████████| 78/78 [00:15<00:00,  5.02it/s, epoch=74/100, avg_loss=0.113]\n",
      "100%|██████████| 78/78 [00:15<00:00,  5.00it/s, epoch=75/100, avg_loss=0.116]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.37it/s, epoch=76/100, avg_loss=0.117]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.45it/s, epoch=77/100, avg_loss=0.115]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.50it/s, epoch=78/100, avg_loss=0.117]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.43it/s, epoch=79/100, avg_loss=0.114]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.69it/s, epoch=80/100, avg_loss=0.112]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.32it/s, epoch=81/100, avg_loss=0.116]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.57it/s, epoch=82/100, avg_loss=0.112]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.48it/s, epoch=83/100, avg_loss=0.114]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.51it/s, epoch=84/100, avg_loss=0.108]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.32it/s, epoch=85/100, avg_loss=0.111]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.64it/s, epoch=86/100, avg_loss=0.111]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.58it/s, epoch=87/100, avg_loss=0.111]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.44it/s, epoch=88/100, avg_loss=0.11]  \n",
      "100%|██████████| 78/78 [00:14<00:00,  5.33it/s, epoch=89/100, avg_loss=0.108]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.64it/s, epoch=90/100, avg_loss=0.109]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.58it/s, epoch=91/100, avg_loss=0.111]\n",
      "100%|██████████| 78/78 [00:13<00:00,  5.65it/s, epoch=92/100, avg_loss=0.109] \n",
      "100%|██████████| 78/78 [00:14<00:00,  5.43it/s, epoch=93/100, avg_loss=0.109]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.43it/s, epoch=94/100, avg_loss=0.107]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.35it/s, epoch=95/100, avg_loss=0.108] \n",
      "100%|██████████| 78/78 [00:13<00:00,  5.68it/s, epoch=96/100, avg_loss=0.106]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.42it/s, epoch=97/100, avg_loss=0.106]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.36it/s, epoch=98/100, avg_loss=0.114]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.53it/s, epoch=99/100, avg_loss=0.107]\n",
      "100%|██████████| 78/78 [00:14<00:00,  5.35it/s, epoch=100/100, avg_loss=0.105]\n",
      "[I 2024-03-04 09:22:39,166] Trial 7 finished with value: 0.16249944036567132 and parameters: {'learning_rate': 2.0641558016647146e-05, 'batch_size': 128, 'num_layers': 5, 'residual_channels': 32, 'dilation_cycle_length': 1}. Best is trial 1 with value: 0.07706547251979637.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./model_checkpoints_optuna/model_epoch_100.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 273/312 [04:26<00:38,  1.03it/s, epoch=1/100, avg_loss=0.18] \n",
      "[W 2024-03-04 09:27:05,569] Trial 8 failed with parameters: {'learning_rate': 0.006144956989616047, 'batch_size': 32, 'num_layers': 14, 'residual_channels': 64, 'dilation_cycle_length': 4} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\difonedseg\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Admin\\AppData\\local\\Temp\\ipykernel_25692\\735566430.py\", line 32, in objective\n",
      "    log_interval=100)\n",
      "  File \"C:\\Users\\Admin\\AppData\\local\\Temp\\ipykernel_25692\\2610101303.py\", line 30, in train_model\n",
      "    losses = model.log_prob(signals)\n",
      "  File \"c:\\Users\\Admin\\Desktop\\diffusion_ts\\s\\net\\gaussian_diffusion.py\", line 280, in log_prob\n",
      "    x.reshape(B * T, 1, -1), time, *args, **kwargs\n",
      "  File \"c:\\Users\\Admin\\Desktop\\diffusion_ts\\s\\net\\gaussian_diffusion.py\", line 259, in p_losses\n",
      "    x_recon = self.denoise_fn(x_noisy, t)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\difonedseg\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"c:\\Users\\Admin\\Desktop\\diffusion_ts\\s\\net\\epsilon_theta.py\", line 102, in forward\n",
      "    x, skip_connection = layer(x, diffusion_step)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\difonedseg\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"c:\\Users\\Admin\\Desktop\\diffusion_ts\\s\\net\\epsilon_theta.py\", line 50, in forward\n",
      "    y = self.dilated_conv(y)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\difonedseg\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\difonedseg\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 313, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"c:\\Users\\Admin\\anaconda3\\envs\\difonedseg\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 310, in _conv_forward\n",
      "    self.padding, self.dilation, self.groups)\n",
      "KeyboardInterrupt\n",
      "[W 2024-03-04 09:27:05,596] Trial 8 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\local\\Temp\\ipykernel_25692\\1893667434.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstudy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'minimize'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# Complete the wandb run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mwandb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\difonedseg\\lib\\site-packages\\optuna\\study\\study.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    458\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m             \u001b[0mshow_progress_bar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m         )\n\u001b[0;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\difonedseg\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     74\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m                 \u001b[0mprogress_bar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m             )\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\difonedseg\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mfrozen_trial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\difonedseg\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     ):\n\u001b[1;32m--> 251\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\difonedseg\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m             \u001b[0mvalue_or_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[1;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\local\\Temp\\ipykernel_25692\\735566430.py\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     30\u001b[0m     val_loss = train_model(100, train_loader, num_batches_per_epoch, model, optimizer, \n\u001b[0;32m     31\u001b[0m                 \u001b[0mvalidation_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_save_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./model_checkpoints_optuna/'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                 log_interval=100)  \n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# Log to wandb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\local\\Temp\\ipykernel_25692\\2610101303.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(epochs, train_loader, num_batches_per_epoch, model, optimizer, validation_iter, device, model_save_path, log_interval)\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[0msignals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_entry\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'signals'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m                 \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m                 \u001b[0mcumm_epoch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\diffusion_ts\\s\\net\\gaussian_diffusion.py\u001b[0m in \u001b[0;36mlog_prob\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[0mtime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         loss = self.p_losses(\n\u001b[1;32m--> 280\u001b[1;33m             \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m         )\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\diffusion_ts\\s\\net\\gaussian_diffusion.py\u001b[0m in \u001b[0;36mp_losses\u001b[1;34m(self, x_start, t, noise)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[0mx_noisy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m         \u001b[0mx_recon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdenoise_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_noisy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"l1\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\difonedseg\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\diffusion_ts\\s\\net\\epsilon_theta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, time)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mskip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresidual_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_connection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiffusion_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m             \u001b[0mskip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mskip_connection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\difonedseg\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\diffusion_ts\\s\\net\\epsilon_theta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, diffusion_step)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdiffusion_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdilated_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mgate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\difonedseg\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\difonedseg\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\difonedseg\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    308\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m    309\u001b[0m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[1;32m--> 310\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "    # Complete the wandb run\n",
    "wandb.run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  9\n",
      "Best trial:\n",
      "  Value:  0.07706547251979637\n",
      "  Params: \n",
      "    learning_rate: 0.0007153912999894027\n",
      "    batch_size: 64\n",
      "    num_layers: 15\n",
      "    residual_channels: 16\n",
      "    dilation_cycle_length: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "difonedseg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
