{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexia\\anaconda3\\envs\\oneddif\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "from datagenfordiff import SynthSignalsDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from gluonts.core.component import validated\n",
    "import wandb\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DiffusionEmbedding(nn.Module):\n",
    "    def __init__(self, dim, proj_dim, max_steps=256):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\n",
    "            \"embedding\", self._build_embedding(dim, max_steps), persistent=False\n",
    "        )\n",
    "        self.projection1 = nn.Linear(2*dim , proj_dim)\n",
    "        self.projection2 = nn.Linear(proj_dim, proj_dim)\n",
    "\n",
    "    def forward(self, diffusion_step):\n",
    "        x = self.embedding[diffusion_step]\n",
    "        x = self.projection1(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.projection2(x)\n",
    "        x = F.silu(x)\n",
    "        return x\n",
    "\n",
    "    def _build_embedding(self, dim, max_steps):\n",
    "        steps = torch.arange(max_steps).unsqueeze(1)  # [T,1]\n",
    "        dims = torch.arange(dim).unsqueeze(0)  # [1,dim]\n",
    "        table = steps * 10.0 ** (dims * 4.0 / dim)  # [T,dim]\n",
    "        table = torch.cat([torch.sin(table), torch.cos(table)], dim=1)\n",
    "        return table\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, residual_channels, dilation, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.dilated_conv = nn.Conv1d(\n",
    "            residual_channels,\n",
    "            2 * residual_channels,\n",
    "            3,\n",
    "            padding=(dilation * (3 - 1)) // 2,\n",
    "            dilation=dilation,\n",
    "            padding_mode='reflect',  # Adjust padding mode if necessary\n",
    "        )\n",
    "        self.diffusion_projection = nn.Linear(hidden_size, residual_channels)\n",
    "        self.output_projection = nn.Conv1d(residual_channels, 2 * residual_channels, 1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.output_projection.weight)\n",
    "\n",
    "    def forward(self, x, diffusion_step):\n",
    "        diffusion_step = self.diffusion_projection(diffusion_step).unsqueeze(-1)\n",
    "\n",
    "        y = x + diffusion_step\n",
    "        y = self.dilated_conv(y)\n",
    "\n",
    "        gate, filter = torch.chunk(y, 2, dim=1)\n",
    "        y = torch.sigmoid(gate) * torch.tanh(filter)\n",
    "\n",
    "        y = self.output_projection(y)\n",
    "        y = F.leaky_relu(y, 0.4)\n",
    "        y = self.dropout(y)\n",
    "\n",
    "        residual, skip = torch.chunk(y, 2, dim=1)\n",
    "        return (x + residual) / math.sqrt(2.0), skip\n",
    "\n",
    "class EpsilonThetaClass(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes = 5,\n",
    "        dropout_rate=0.1,\n",
    "        #cond_length,\n",
    "        time_emb_dim=8,\n",
    "        residual_layers=8,\n",
    "        residual_channels=16,\n",
    "        dilation_cycle_length=2,\n",
    "        residual_hidden=16,\n",
    "        class_emb_dim=5,\n",
    "        target_dim=1,\n",
    "        \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.class_embedding = nn.Embedding(num_classes, class_emb_dim)\n",
    "        \n",
    "        self.input_projection = nn.Conv1d(\n",
    "            1+class_emb_dim, residual_channels, 1, padding=2, padding_mode=\"zeros\"\n",
    "        )\n",
    "        self.diffusion_embedding = DiffusionEmbedding(\n",
    "            time_emb_dim, proj_dim=residual_hidden\n",
    "        )\n",
    "        self.residual_layers = nn.ModuleList()\n",
    "        for i in range(residual_layers):\n",
    "            # Cycle through a range of dilation rates\n",
    "            dilation = 2 ** (i % dilation_cycle_length)\n",
    "            self.residual_layers.append(ResidualBlock(\n",
    "                hidden_size=residual_hidden,\n",
    "                residual_channels=residual_channels,\n",
    "                dilation=dilation,\n",
    "                dropout_rate=dropout_rate,  # Pass the dropout rate to each block\n",
    "            ))\n",
    "        self.skip_projection = nn.Conv1d(residual_channels, residual_channels, 3)\n",
    "        self.output_projection = nn.Conv1d(residual_channels, target_dim, 3)\n",
    "\n",
    "        nn.init.kaiming_normal_(self.input_projection.weight)\n",
    "        nn.init.kaiming_normal_(self.skip_projection.weight)\n",
    "        nn.init.kaiming_normal_(self.output_projection.weight)\n",
    "\n",
    "    def forward(self, inputs, time, class_labels):\n",
    "        class_embeddings = self.class_embedding(class_labels)  # [batch_size, class_emb_dim]\n",
    "        class_embeddings = class_embeddings.unsqueeze(2).expand(-1, -1, inputs.size(2))\n",
    "\n",
    "        # Concatenate class embeddings with inputs\n",
    "        inputs = torch.cat([inputs, class_embeddings], dim=1)\n",
    "\n",
    "        x = self.input_projection(inputs)\n",
    "        x = F.leaky_relu(x, 0.4)\n",
    "\n",
    "        diffusion_step = self.diffusion_embedding(time)\n",
    "        skip = []\n",
    "        for layer in self.residual_layers:\n",
    "            x, skip_connection = layer(x, diffusion_step)\n",
    "            skip.append(skip_connection)\n",
    "\n",
    "        x = torch.sum(torch.stack(skip), dim=0) / math.sqrt(len(self.residual_layers))\n",
    "        x = self.skip_projection(x)\n",
    "        x = F.leaky_relu(x, 0.4)\n",
    "        x = self.output_projection(x)\n",
    "        return x\n",
    "\n",
    "from functools import partial\n",
    "from inspect import isfunction\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "#  COND HAS BEEN TAKEN OUT FROM FUNCTIONS\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    if val is not None:\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "\n",
    "def noise_like(shape, device, repeat=False):\n",
    "    repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(\n",
    "        shape[0], *((1,) * (len(shape) - 1))\n",
    "    )\n",
    "    noise = lambda: torch.randn(shape, device=device)\n",
    "    return repeat_noise() if repeat else noise()\n",
    "\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"\n",
    "    cosine schedule\n",
    "    as proposed in https://openreview.model/forum?id=-NEXDKk8gZ\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = np.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = np.cos(((x / timesteps) + s) / (1 + s) * np.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return np.clip(betas, 0, 0.999)\n",
    "\n",
    "\n",
    "class GaussianDiffusionClass(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        denoise_fn,#pass epsilon theta\n",
    "        input_size,\n",
    "        beta_end=0.1,\n",
    "        diff_steps=100,\n",
    "        loss_type=\"l2\",\n",
    "        betas=None,\n",
    "        beta_schedule=\"linear\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.denoise_fn = denoise_fn\n",
    "        self.input_size = input_size\n",
    "        self.__scale = None\n",
    "\n",
    "        if betas is not None:\n",
    "            betas = (\n",
    "                betas.detach().cpu().numpy()\n",
    "                if isinstance(betas, torch.Tensor)\n",
    "                else betas\n",
    "            )\n",
    "        else:\n",
    "            if beta_schedule == \"linear\":\n",
    "                betas = np.linspace(1e-4, beta_end, diff_steps)\n",
    "            elif beta_schedule == \"quad\":\n",
    "                betas = np.linspace(1e-4 ** 0.5, beta_end ** 0.5, diff_steps) ** 2\n",
    "            elif beta_schedule == \"const\":\n",
    "                betas = beta_end * np.ones(diff_steps)\n",
    "            elif beta_schedule == \"jsd\":  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n",
    "                betas = 1.0 / np.linspace(diff_steps, 1, diff_steps)\n",
    "            elif beta_schedule == \"sigmoid\":\n",
    "                betas = np.linspace(-6, 6, diff_steps)\n",
    "                betas = (beta_end - 1e-4) / (np.exp(-betas) + 1) + 1e-4\n",
    "            elif beta_schedule == \"cosine\":\n",
    "                betas = cosine_beta_schedule(diff_steps)\n",
    "            else:\n",
    "                raise NotImplementedError(beta_schedule)\n",
    "\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = np.cumprod(alphas, axis=0)\n",
    "        alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])\n",
    "\n",
    "        (timesteps,) = betas.shape\n",
    "        self.num_timesteps = int(timesteps)\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "        to_torch = partial(torch.tensor, dtype=torch.float32)\n",
    "\n",
    "        self.register_buffer(\"betas\", to_torch(betas))\n",
    "        self.register_buffer(\"alphas_cumprod\", to_torch(alphas_cumprod))\n",
    "        self.register_buffer(\"alphas_cumprod_prev\", to_torch(alphas_cumprod_prev))\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "        self.register_buffer(\"sqrt_alphas_cumprod\", to_torch(np.sqrt(alphas_cumprod)))\n",
    "        self.register_buffer(\n",
    "            \"sqrt_one_minus_alphas_cumprod\", to_torch(np.sqrt(1.0 - alphas_cumprod))\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"log_one_minus_alphas_cumprod\", to_torch(np.log(1.0 - alphas_cumprod))\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"sqrt_recip_alphas_cumprod\", to_torch(np.sqrt(1.0 / alphas_cumprod))\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"sqrt_recipm1_alphas_cumprod\", to_torch(np.sqrt(1.0 / alphas_cumprod - 1))\n",
    "        )\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        posterior_variance = (\n",
    "            betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
    "        )\n",
    "        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n",
    "        self.register_buffer(\"posterior_variance\", to_torch(posterior_variance))\n",
    "        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n",
    "        self.register_buffer(\n",
    "            \"posterior_log_variance_clipped\",\n",
    "            to_torch(np.log(np.maximum(posterior_variance, 1e-20))),\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"posterior_mean_coef1\",\n",
    "            to_torch(betas * np.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod)),\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"posterior_mean_coef2\",\n",
    "            to_torch(\n",
    "                (1.0 - alphas_cumprod_prev) * np.sqrt(alphas) / (1.0 - alphas_cumprod)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def scale(self):\n",
    "        return self.__scale\n",
    "\n",
    "    @scale.setter\n",
    "    def scale(self, scale):\n",
    "        self.__scale = scale\n",
    "\n",
    "    def q_mean_variance(self, x_start, t):\n",
    "        mean = extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "        variance = extract(1.0 - self.alphas_cumprod, t, x_start.shape)\n",
    "        log_variance = extract(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "        return mean, variance, log_variance\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        return (\n",
    "            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n",
    "            - extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "        )\n",
    "\n",
    "    def q_posterior(self, x_start, x_t, t):\n",
    "        posterior_mean = (\n",
    "            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start\n",
    "            + extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = extract(\n",
    "            self.posterior_log_variance_clipped, t, x_t.shape\n",
    "        )\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def p_mean_variance(self, x, t, class_labels, clip_denoised: bool):\n",
    "        x_recon = self.predict_start_from_noise(\n",
    "            x, t=t, noise=self.denoise_fn(x, t, class_labels)\n",
    "        )\n",
    "\n",
    "        if clip_denoised:\n",
    "            x_recon.clamp_(-1.0, 1.0)\n",
    "\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(\n",
    "            x_start=x_recon, x_t=x, t=t\n",
    "        )\n",
    "        return model_mean, posterior_variance, posterior_log_variance\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def q_sample_loop(self,x_0, shape):\n",
    "        device = self.betas.device\n",
    "\n",
    "        b=shape[0]\n",
    "        img=torch.empty(self.num_timesteps, *shape)\n",
    "        for i in range(0, self.num_timesteps) :\n",
    "            img[i]=self.q_sample(x_0, torch.full((b,), i, device=device, dtype=torch.long))\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t, class_labels, clip_denoised=False, repeat_noise=False):\n",
    "        b, *_, device = *x.shape, x.device\n",
    "        model_mean, _, model_log_variance = self.p_mean_variance(\n",
    "            x=x, t=t, class_labels = class_labels, clip_denoised=clip_denoised\n",
    "        )\n",
    "        noise = noise_like(x.shape, device, repeat_noise)\n",
    "        # no noise when t == 0\n",
    "        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n",
    "        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, x, class_labels):\n",
    "        device = self.betas.device\n",
    "\n",
    "        b = x.shape[0]\n",
    "        img = torch.randn(x.shape, device=device)\n",
    "\n",
    "        for i in reversed(range(0, self.num_timesteps)):\n",
    "            img = self.p_sample(\n",
    "                img, torch.full((b,), i, device=device, dtype=torch.long), class_labels\n",
    "            )\n",
    "        return img\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, class_labels, sample_shape=torch.Size(), cond=None):\n",
    "        if cond is not None:\n",
    "            shape = cond.shape[:-1] + (self.input_size,)\n",
    "            # TODO reshape cond to (B*T, 1, -1)\n",
    "        else:\n",
    "            shape = sample_shape\n",
    "        x_hat = self.p_sample_loop(shape, class_labels, cond)  # TODO reshape x_hat to (B,T,-1)\n",
    "\n",
    "        if self.scale is not None:\n",
    "            x_hat *= self.scale\n",
    "        return x_hat\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def interpolate(self, x1, x2,class_labels, t=None, lam=0.5):\n",
    "        b, *_, device = *x1.shape, x1.device\n",
    "        t = default(t, self.num_timesteps - 1)\n",
    "\n",
    "        assert x1.shape == x2.shape\n",
    "\n",
    "        t_batched = torch.stack([torch.tensor(t, device=device)] * b)\n",
    "        xt1, xt2 = map(lambda x: self.q_sample(x, t=t_batched), (x1, x2))\n",
    "\n",
    "        img = (1 - lam) * xt1 + lam * xt2\n",
    "        for i in reversed(range(0, t)):\n",
    "            img = self.p_sample(\n",
    "                img, torch.full((b,), i, device=device, dtype=torch.long), class_labels\n",
    "            )\n",
    "\n",
    "        return img\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "            + extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
    "        )\n",
    "\n",
    "    def p_losses(self, x_start, t, class_labels, noise=None):\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "        x_recon = self.denoise_fn(x_noisy, t, class_labels)\n",
    "\n",
    "        if self.loss_type == \"l1\":\n",
    "            loss = F.l1_loss(x_recon, noise)\n",
    "        elif self.loss_type == \"l2\":\n",
    "            loss = F.mse_loss(x_recon, noise)\n",
    "        elif self.loss_type == \"huber\":\n",
    "            loss = F.smooth_l1_loss(x_recon, noise)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def log_prob(self, x, class_labels,*args, **kwargs):\n",
    "        if self.scale is not None:\n",
    "            x /= self.scale\n",
    "\n",
    "        B, T, _ = x.shape\n",
    "\n",
    "        time = torch.randint(0, self.num_timesteps, (B * T,), device=x.device).long()\n",
    "        loss = self.p_losses(\n",
    "            x.reshape(B * T, 1, -1),  time, class_labels,*args, **kwargs\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            net: GaussianDiffusionClass,\n",
    "            epochs: int = 500,\n",
    "            batch_size: int = 32,\n",
    "            num_batches_per_epoch: int = 50,\n",
    "            learning_rate: float = 1e-3,\n",
    "            weight_decay: float = 1e-6,\n",
    "            maximum_learning_rate: float = 1e-2,\n",
    "            model_name : str = 'model',\n",
    "            model_type : str ='torch',\n",
    "            model_save_path : str = 'model_sav_path',\n",
    "            input_size = [256],\n",
    "           \n",
    "\n",
    "            **kwargs,\n",
    "    )->None:\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches_per_epoch = num_batches_per_epoch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.maximum_learning_rate = maximum_learning_rate\n",
    "        self.model_name = model_name\n",
    "        self.model_type = model_type\n",
    "        self.model_save_path = model_save_path\n",
    "        self.input_size = input_size\n",
    "        self.net = net\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    def __call__(\n",
    "            self,\n",
    "           \n",
    "            train_iter: DataLoader,\n",
    " \n",
    "    )->None:\n",
    "        \n",
    "        wandb.login()\n",
    "        \n",
    "        wandb.init(project=\"test_train_class\")\n",
    "\n",
    "        # Log hyperparameters and other configurations\n",
    "        config = {\n",
    "            'epochs': self.epochs,\n",
    "            'batch_size': self.batch_size,\n",
    "            'num_batches_per_epoch': self.num_batches_per_epoch,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'weight_decay': self.weight_decay,\n",
    "            'maximum_learning_rate': self.maximum_learning_rate,\n",
    "        }\n",
    "        wandb.config.update(config)\n",
    "\n",
    "        optimizer = Adam(\n",
    "            self.net.parameters(), lr = self.learning_rate, weight_decay = self.weight_decay\n",
    "        )\n",
    "\n",
    "        lr_scheduler = OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr = self.maximum_learning_rate,\n",
    "            steps_per_epoch = self.num_batches_per_epoch,\n",
    "            epochs = self.epochs,\n",
    "        ) \n",
    "\n",
    "        losses_t = []\n",
    "        for epoch in range(self.epochs):\n",
    "            tic = time.time()\n",
    "            cumm_epoch_loss = 0.0\n",
    "\n",
    "            with tqdm(train_iter, total=self.num_batches_per_epoch - 1) as it:\n",
    "                for batch_no, data_entry in enumerate(it, start=1):\n",
    "                    optimizer.zero_grad()\n",
    "                    signals = data_entry['signals']\n",
    "                    class_labels = data_entry['sc']  # Ensure this is included in your model's log_prob method\n",
    "                    losses = self.net.log_prob(signals, class_labels=class_labels)\n",
    "                    cumm_epoch_loss += losses.item()\n",
    "\n",
    "                    avg_epoch_loss = cumm_epoch_loss / batch_no\n",
    "                    it.set_postfix({\"epoch\": f\"{epoch + 1}/{self.epochs}\", \"avg_loss\": avg_epoch_loss}, refresh=False)\n",
    "\n",
    "                    wandb.log({\"train_loss\": losses.item()})\n",
    "                    losses.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                \n",
    "\n",
    "                    if self.num_batches_per_epoch == batch_no:\n",
    "                        break\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": avg_epoch_loss,\n",
    "                \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "                \"gradient_norm\": self.calculate_gradient_norm(self.net),\n",
    "                \"training_time_per_epoch\": time.time() - tic,\n",
    "            })\n",
    "            losses_t.append(avg_epoch_loss)\n",
    "        \n",
    "        self.save_model_as_artifact(self.net)\n",
    "            \n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_gradient_norm(net):\n",
    "        total_norm = 0.0\n",
    "        for param in net.parameters():\n",
    "            if param.grad is not None:\n",
    "                param_norm = param.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** 0.5\n",
    "        return total_norm\n",
    "    \n",
    "    def save_model_as_artifact(self, model):\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        # Save the model state dictionary instead of using ONNX\n",
    "        \n",
    "        if not os.path.exists(self.model_save_path):\n",
    "            os.makedirs(self.model_save_path)\n",
    "        model_path = os.path.join(self.model_save_path, f'{self.model_name}.pth')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        # Create an artifact for logging to wandb\n",
    "        artifact = wandb.Artifact(self.model_name, type=self.model_type)\n",
    "        \n",
    "        # Add metadata to the artifact\n",
    "        artifact.metadata = {\n",
    "            'format': 'pytorch_state_dict',\n",
    "            'model_type': self.model_type,\n",
    "            'epochs': self.epochs,\n",
    "            'batch_size': self.batch_size,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'weight_decay': self.weight_decay,\n",
    "            'layers': [str(layer) for layer in model.children()],\n",
    "        }\n",
    "        \n",
    "        # Add the model file to the artifact\n",
    "        artifact.add_file(model_path)\n",
    "\n",
    "        # Log the artifact to wandb\n",
    "        wandb.log_artifact(artifact)\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "        \"\"\"\n",
    "        Custom collate function to reshape data into [batch size, channels, size].\n",
    "        \"\"\"\n",
    "        # Assuming your signals are originally in the shape [size]\n",
    "        # and you want to add a single channel dimension\n",
    "        signals = torch.stack([item['signals'] for item in batch]).unsqueeze(1)  # Adds a channel dimension\n",
    "        gt = torch.stack([item['gt'] for item in batch])\n",
    "        sc = torch.stack([item['sc'] for item in batch])\n",
    "        \n",
    "        return {'signals': signals, 'gt': gt, 'sc': sc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Alexia\\diffusion_ts\\wandb\\run-20240428_183538-i1g2826h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fyp_a/test_train_class/runs/i1g2826h/workspace' target=\"_blank\">reshid16_resch16_batch64_lr0.001_e200</a></strong> to <a href='https://wandb.ai/fyp_a/test_train_class' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fyp_a/test_train_class' target=\"_blank\">https://wandb.ai/fyp_a/test_train_class</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fyp_a/test_train_class/runs/i1g2826h/workspace' target=\"_blank\">https://wandb.ai/fyp_a/test_train_class/runs/i1g2826h/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:i1g2826h) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">reshid16_resch16_batch64_lr0.001_e200</strong> at: <a href='https://wandb.ai/fyp_a/test_train_class/runs/i1g2826h/workspace' target=\"_blank\">https://wandb.ai/fyp_a/test_train_class/runs/i1g2826h/workspace</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240428_183538-i1g2826h\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:i1g2826h). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Alexia\\diffusion_ts\\wandb\\run-20240428_183539-cfzd1lxn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fyp_a/test_train_class/runs/cfzd1lxn/workspace' target=\"_blank\">brisk-surf-44</a></strong> to <a href='https://wandb.ai/fyp_a/test_train_class' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fyp_a/test_train_class' target=\"_blank\">https://wandb.ai/fyp_a/test_train_class</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fyp_a/test_train_class/runs/cfzd1lxn/workspace' target=\"_blank\">https://wandb.ai/fyp_a/test_train_class/runs/cfzd1lxn/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [00:06<00:00,  7.96it/s, epoch=1/200, avg_loss=0.643]\n",
      "100%|██████████| 49/49 [00:02<00:00, 22.29it/s, epoch=2/200, avg_loss=0.335]\n",
      "100%|██████████| 49/49 [00:02<00:00, 16.89it/s, epoch=3/200, avg_loss=0.298]\n",
      "100%|██████████| 49/49 [00:04<00:00, 11.84it/s, epoch=4/200, avg_loss=0.293]\n",
      "100%|██████████| 49/49 [00:04<00:00, 10.39it/s, epoch=5/200, avg_loss=0.276]\n",
      "100%|██████████| 49/49 [00:05<00:00,  9.75it/s, epoch=6/200, avg_loss=0.27] \n",
      "100%|██████████| 49/49 [00:05<00:00,  9.58it/s, epoch=7/200, avg_loss=0.272]\n",
      "100%|██████████| 49/49 [00:02<00:00, 16.68it/s, epoch=8/200, avg_loss=0.253]\n",
      "100%|██████████| 49/49 [00:04<00:00, 10.80it/s, epoch=9/200, avg_loss=0.247]\n",
      "100%|██████████| 49/49 [00:02<00:00, 21.01it/s, epoch=10/200, avg_loss=0.256]\n",
      "100%|██████████| 49/49 [00:02<00:00, 20.57it/s, epoch=11/200, avg_loss=0.244]\n",
      "100%|██████████| 49/49 [00:05<00:00,  9.33it/s, epoch=12/200, avg_loss=0.23] \n",
      "100%|██████████| 49/49 [00:05<00:00,  9.45it/s, epoch=13/200, avg_loss=0.225]\n",
      "100%|██████████| 49/49 [00:06<00:00,  7.51it/s, epoch=14/200, avg_loss=0.225]\n",
      "100%|██████████| 49/49 [00:05<00:00,  8.98it/s, epoch=15/200, avg_loss=0.226]\n",
      "100%|██████████| 49/49 [00:04<00:00, 10.18it/s, epoch=16/200, avg_loss=0.221]\n",
      "100%|██████████| 49/49 [00:07<00:00,  6.35it/s, epoch=17/200, avg_loss=0.214]\n",
      "100%|██████████| 49/49 [00:06<00:00,  7.46it/s, epoch=18/200, avg_loss=0.212]\n",
      "100%|██████████| 49/49 [00:05<00:00,  8.98it/s, epoch=19/200, avg_loss=0.2]  \n",
      "100%|██████████| 49/49 [00:08<00:00,  5.75it/s, epoch=20/200, avg_loss=0.191]\n",
      "100%|██████████| 49/49 [00:05<00:00,  8.58it/s, epoch=21/200, avg_loss=0.192]\n",
      "100%|██████████| 49/49 [00:06<00:00,  7.15it/s, epoch=22/200, avg_loss=0.182]\n",
      "100%|██████████| 49/49 [00:06<00:00,  7.98it/s, epoch=23/200, avg_loss=0.177]\n",
      "100%|██████████| 49/49 [00:04<00:00, 11.67it/s, epoch=24/200, avg_loss=0.173]\n",
      "100%|██████████| 49/49 [00:03<00:00, 14.87it/s, epoch=25/200, avg_loss=0.167]\n",
      "100%|██████████| 49/49 [00:05<00:00,  9.43it/s, epoch=26/200, avg_loss=0.164]\n",
      "100%|██████████| 49/49 [00:05<00:00,  9.48it/s, epoch=27/200, avg_loss=0.165]\n",
      "100%|██████████| 49/49 [00:03<00:00, 16.13it/s, epoch=28/200, avg_loss=0.161]\n",
      "100%|██████████| 49/49 [00:02<00:00, 18.62it/s, epoch=29/200, avg_loss=0.156]\n",
      "100%|██████████| 49/49 [00:03<00:00, 15.73it/s, epoch=30/200, avg_loss=0.154]\n",
      "100%|██████████| 49/49 [00:02<00:00, 16.86it/s, epoch=31/200, avg_loss=0.155]\n",
      "100%|██████████| 49/49 [00:03<00:00, 12.42it/s, epoch=32/200, avg_loss=0.153]\n",
      "100%|██████████| 49/49 [00:03<00:00, 14.34it/s, epoch=33/200, avg_loss=0.151]\n",
      "100%|██████████| 49/49 [00:04<00:00, 10.71it/s, epoch=34/200, avg_loss=0.144]\n",
      "100%|██████████| 49/49 [00:11<00:00,  4.36it/s, epoch=35/200, avg_loss=0.143]\n",
      "100%|██████████| 49/49 [00:10<00:00,  4.88it/s, epoch=36/200, avg_loss=0.148]\n",
      "100%|██████████| 49/49 [00:07<00:00,  6.31it/s, epoch=37/200, avg_loss=0.145]\n",
      "100%|██████████| 49/49 [00:04<00:00,  9.90it/s, epoch=38/200, avg_loss=0.146]\n",
      "100%|██████████| 49/49 [00:03<00:00, 12.70it/s, epoch=39/200, avg_loss=0.145]\n",
      "100%|██████████| 49/49 [00:03<00:00, 13.16it/s, epoch=40/200, avg_loss=0.143]\n",
      "100%|██████████| 49/49 [00:02<00:00, 19.88it/s, epoch=41/200, avg_loss=0.138]\n",
      "100%|██████████| 49/49 [00:02<00:00, 17.19it/s, epoch=42/200, avg_loss=0.142]\n",
      "100%|██████████| 49/49 [00:03<00:00, 16.07it/s, epoch=43/200, avg_loss=0.14] \n",
      "100%|██████████| 49/49 [00:03<00:00, 12.96it/s, epoch=44/200, avg_loss=0.135]\n",
      "100%|██████████| 49/49 [00:03<00:00, 14.43it/s, epoch=45/200, avg_loss=0.138]\n",
      "100%|██████████| 49/49 [00:02<00:00, 17.18it/s, epoch=46/200, avg_loss=0.133]\n",
      "100%|██████████| 49/49 [00:03<00:00, 14.79it/s, epoch=47/200, avg_loss=0.136]\n",
      "100%|██████████| 49/49 [00:02<00:00, 19.88it/s, epoch=48/200, avg_loss=0.134]\n",
      "100%|██████████| 49/49 [00:04<00:00, 11.40it/s, epoch=49/200, avg_loss=0.132]\n",
      "100%|██████████| 49/49 [00:07<00:00,  6.61it/s, epoch=50/200, avg_loss=0.134]\n",
      "100%|██████████| 49/49 [00:04<00:00, 12.20it/s, epoch=51/200, avg_loss=0.133]\n",
      "100%|██████████| 49/49 [00:03<00:00, 14.12it/s, epoch=52/200, avg_loss=0.13] \n",
      "100%|██████████| 49/49 [00:04<00:00, 12.10it/s, epoch=53/200, avg_loss=0.129]\n",
      "100%|██████████| 49/49 [00:02<00:00, 22.43it/s, epoch=54/200, avg_loss=0.128]\n",
      "100%|██████████| 49/49 [00:03<00:00, 13.08it/s, epoch=55/200, avg_loss=0.128]\n",
      "100%|██████████| 49/49 [00:04<00:00,  9.80it/s, epoch=56/200, avg_loss=0.125]\n",
      "100%|██████████| 49/49 [00:06<00:00,  7.52it/s, epoch=57/200, avg_loss=0.125]\n",
      "100%|██████████| 49/49 [00:03<00:00, 14.21it/s, epoch=58/200, avg_loss=0.125]\n",
      "100%|██████████| 49/49 [00:08<00:00,  5.81it/s, epoch=59/200, avg_loss=0.127]\n",
      "100%|██████████| 49/49 [00:03<00:00, 14.41it/s, epoch=60/200, avg_loss=0.127]\n",
      "100%|██████████| 49/49 [00:03<00:00, 15.40it/s, epoch=61/200, avg_loss=0.123]\n",
      "100%|██████████| 49/49 [00:03<00:00, 12.62it/s, epoch=62/200, avg_loss=0.124]\n",
      "100%|██████████| 49/49 [00:02<00:00, 18.83it/s, epoch=63/200, avg_loss=0.124]\n",
      "100%|██████████| 49/49 [00:03<00:00, 13.85it/s, epoch=64/200, avg_loss=0.123]\n",
      "100%|██████████| 49/49 [00:02<00:00, 16.36it/s, epoch=65/200, avg_loss=0.122]\n",
      "100%|██████████| 49/49 [00:02<00:00, 21.98it/s, epoch=66/200, avg_loss=0.121]\n",
      "100%|██████████| 49/49 [00:02<00:00, 18.07it/s, epoch=67/200, avg_loss=0.122]\n",
      "100%|██████████| 49/49 [00:05<00:00,  9.15it/s, epoch=68/200, avg_loss=0.119]\n",
      "100%|██████████| 49/49 [00:02<00:00, 16.60it/s, epoch=69/200, avg_loss=0.122]\n",
      "100%|██████████| 49/49 [00:02<00:00, 21.93it/s, epoch=70/200, avg_loss=0.122]\n",
      "100%|██████████| 49/49 [00:04<00:00, 11.68it/s, epoch=71/200, avg_loss=0.122]\n",
      "100%|██████████| 49/49 [00:04<00:00, 11.89it/s, epoch=72/200, avg_loss=0.121]\n",
      "100%|██████████| 49/49 [00:02<00:00, 16.52it/s, epoch=73/200, avg_loss=0.118]\n",
      "100%|██████████| 49/49 [00:02<00:00, 16.36it/s, epoch=74/200, avg_loss=0.12] \n",
      "100%|██████████| 49/49 [00:02<00:00, 19.07it/s, epoch=75/200, avg_loss=0.117]\n",
      "100%|██████████| 49/49 [00:02<00:00, 20.41it/s, epoch=76/200, avg_loss=0.117]\n",
      "100%|██████████| 49/49 [00:02<00:00, 19.08it/s, epoch=77/200, avg_loss=0.116]\n",
      "100%|██████████| 49/49 [00:03<00:00, 16.06it/s, epoch=78/200, avg_loss=0.116]\n",
      "100%|██████████| 49/49 [00:02<00:00, 22.16it/s, epoch=79/200, avg_loss=0.113]\n",
      "100%|██████████| 49/49 [00:03<00:00, 14.39it/s, epoch=80/200, avg_loss=0.115]\n",
      "100%|██████████| 49/49 [00:02<00:00, 17.40it/s, epoch=81/200, avg_loss=0.118]\n",
      "100%|██████████| 49/49 [00:05<00:00,  9.69it/s, epoch=82/200, avg_loss=0.115]\n",
      "100%|██████████| 49/49 [00:07<00:00,  6.76it/s, epoch=83/200, avg_loss=0.118]\n",
      "100%|██████████| 49/49 [00:06<00:00,  7.79it/s, epoch=84/200, avg_loss=0.112]\n",
      "100%|██████████| 49/49 [00:03<00:00, 15.25it/s, epoch=85/200, avg_loss=0.116]\n",
      "100%|██████████| 49/49 [00:03<00:00, 15.05it/s, epoch=86/200, avg_loss=0.115]\n",
      "100%|██████████| 49/49 [00:02<00:00, 16.79it/s, epoch=87/200, avg_loss=0.114]\n",
      "100%|██████████| 49/49 [00:03<00:00, 13.19it/s, epoch=88/200, avg_loss=0.113]\n",
      "100%|██████████| 49/49 [00:03<00:00, 16.30it/s, epoch=89/200, avg_loss=0.111]\n",
      "100%|██████████| 49/49 [00:04<00:00, 11.36it/s, epoch=90/200, avg_loss=0.113]\n",
      "100%|██████████| 49/49 [00:02<00:00, 18.58it/s, epoch=91/200, avg_loss=0.115]\n",
      "100%|██████████| 49/49 [00:03<00:00, 12.37it/s, epoch=92/200, avg_loss=0.114]\n",
      "100%|██████████| 49/49 [00:03<00:00, 12.25it/s, epoch=93/200, avg_loss=0.114]\n",
      "100%|██████████| 49/49 [00:03<00:00, 14.06it/s, epoch=94/200, avg_loss=0.109]\n",
      "100%|██████████| 49/49 [00:05<00:00,  8.28it/s, epoch=95/200, avg_loss=0.11] \n",
      "100%|██████████| 49/49 [00:02<00:00, 16.98it/s, epoch=96/200, avg_loss=0.111]\n",
      "100%|██████████| 49/49 [00:05<00:00,  8.32it/s, epoch=97/200, avg_loss=0.11] \n",
      "100%|██████████| 49/49 [00:05<00:00,  8.53it/s, epoch=98/200, avg_loss=0.109]\n",
      "100%|██████████| 49/49 [00:04<00:00, 10.80it/s, epoch=99/200, avg_loss=0.108]\n",
      "100%|██████████| 49/49 [00:03<00:00, 14.21it/s, epoch=100/200, avg_loss=0.111]\n",
      "100%|██████████| 49/49 [00:02<00:00, 21.52it/s, epoch=101/200, avg_loss=0.109]\n",
      "100%|██████████| 49/49 [00:04<00:00, 11.30it/s, epoch=102/200, avg_loss=0.11] \n",
      "100%|██████████| 49/49 [00:04<00:00, 10.18it/s, epoch=103/200, avg_loss=0.11] \n",
      "100%|██████████| 49/49 [00:05<00:00,  9.11it/s, epoch=104/200, avg_loss=0.111]\n",
      "100%|██████████| 49/49 [00:07<00:00,  6.95it/s, epoch=105/200, avg_loss=0.11] \n",
      "100%|██████████| 49/49 [00:02<00:00, 17.04it/s, epoch=106/200, avg_loss=0.109]\n",
      "100%|██████████| 49/49 [00:03<00:00, 14.50it/s, epoch=107/200, avg_loss=0.11] \n",
      "100%|██████████| 49/49 [00:05<00:00,  8.65it/s, epoch=108/200, avg_loss=0.109]\n",
      "100%|██████████| 49/49 [00:05<00:00,  8.71it/s, epoch=109/200, avg_loss=0.108]\n",
      "100%|██████████| 49/49 [00:06<00:00,  7.93it/s, epoch=110/200, avg_loss=0.108]\n",
      "100%|██████████| 49/49 [00:06<00:00,  7.85it/s, epoch=111/200, avg_loss=0.105]\n",
      "100%|██████████| 49/49 [00:02<00:00, 21.56it/s, epoch=112/200, avg_loss=0.106]\n",
      "100%|██████████| 49/49 [00:03<00:00, 15.71it/s, epoch=113/200, avg_loss=0.108]\n",
      "100%|██████████| 49/49 [00:02<00:00, 21.77it/s, epoch=114/200, avg_loss=0.107]\n",
      "100%|██████████| 49/49 [00:02<00:00, 20.55it/s, epoch=115/200, avg_loss=0.106]\n",
      "100%|██████████| 49/49 [00:04<00:00, 10.67it/s, epoch=116/200, avg_loss=0.107]\n",
      "100%|██████████| 49/49 [00:04<00:00, 10.33it/s, epoch=117/200, avg_loss=0.109]\n",
      "100%|██████████| 49/49 [00:05<00:00,  9.54it/s, epoch=118/200, avg_loss=0.106]\n",
      "100%|██████████| 49/49 [00:04<00:00, 11.59it/s, epoch=119/200, avg_loss=0.105]\n",
      "100%|██████████| 49/49 [00:05<00:00,  8.67it/s, epoch=120/200, avg_loss=0.104]\n",
      "100%|██████████| 49/49 [00:03<00:00, 12.84it/s, epoch=121/200, avg_loss=0.104]\n",
      "100%|██████████| 49/49 [00:04<00:00, 11.45it/s, epoch=122/200, avg_loss=0.105]\n",
      "100%|██████████| 49/49 [00:03<00:00, 12.38it/s, epoch=123/200, avg_loss=0.108]\n",
      "100%|██████████| 49/49 [00:05<00:00,  8.80it/s, epoch=124/200, avg_loss=0.104]\n",
      "100%|██████████| 49/49 [00:06<00:00,  7.02it/s, epoch=125/200, avg_loss=0.105] \n",
      "100%|██████████| 49/49 [00:05<00:00,  8.34it/s, epoch=126/200, avg_loss=0.105]\n",
      "100%|██████████| 49/49 [00:02<00:00, 17.91it/s, epoch=127/200, avg_loss=0.106]\n",
      "100%|██████████| 49/49 [00:03<00:00, 13.85it/s, epoch=128/200, avg_loss=0.105]\n",
      "100%|██████████| 49/49 [00:06<00:00,  7.57it/s, epoch=129/200, avg_loss=0.104]\n",
      "100%|██████████| 49/49 [00:07<00:00,  6.26it/s, epoch=130/200, avg_loss=0.105]\n",
      "100%|██████████| 49/49 [00:04<00:00, 10.22it/s, epoch=131/200, avg_loss=0.104]\n",
      "100%|██████████| 49/49 [00:05<00:00,  9.28it/s, epoch=132/200, avg_loss=0.103]\n",
      "100%|██████████| 49/49 [00:05<00:00,  9.01it/s, epoch=133/200, avg_loss=0.104]\n",
      "100%|██████████| 49/49 [00:03<00:00, 15.01it/s, epoch=134/200, avg_loss=0.103]\n",
      "100%|██████████| 49/49 [00:04<00:00, 11.32it/s, epoch=135/200, avg_loss=0.105]\n",
      "100%|██████████| 49/49 [00:03<00:00, 13.61it/s, epoch=136/200, avg_loss=0.104]\n",
      "100%|██████████| 49/49 [00:06<00:00,  8.01it/s, epoch=137/200, avg_loss=0.104]\n",
      "100%|██████████| 49/49 [00:02<00:00, 18.15it/s, epoch=138/200, avg_loss=0.104]\n",
      "100%|██████████| 49/49 [00:05<00:00,  8.95it/s, epoch=139/200, avg_loss=0.102] \n",
      "100%|██████████| 49/49 [00:05<00:00,  8.63it/s, epoch=140/200, avg_loss=0.102]\n",
      "100%|██████████| 49/49 [00:10<00:00,  4.50it/s, epoch=141/200, avg_loss=0.103]\n",
      "100%|██████████| 49/49 [00:42<00:00,  1.16it/s, epoch=142/200, avg_loss=0.102] \n",
      "100%|██████████| 49/49 [00:03<00:00, 13.32it/s, epoch=143/200, avg_loss=0.102]\n",
      "100%|██████████| 49/49 [00:04<00:00, 12.20it/s, epoch=144/200, avg_loss=0.103]\n",
      "100%|██████████| 49/49 [00:04<00:00, 10.22it/s, epoch=145/200, avg_loss=0.105]\n",
      "100%|██████████| 49/49 [00:02<00:00, 20.19it/s, epoch=146/200, avg_loss=0.104]\n",
      "100%|██████████| 49/49 [00:04<00:00, 11.56it/s, epoch=147/200, avg_loss=0.102] \n",
      "100%|██████████| 49/49 [00:02<00:00, 18.48it/s, epoch=148/200, avg_loss=0.101] \n",
      "100%|██████████| 49/49 [00:02<00:00, 17.22it/s, epoch=149/200, avg_loss=0.102]\n",
      "100%|██████████| 49/49 [00:05<00:00,  8.42it/s, epoch=150/200, avg_loss=0.103] \n",
      "100%|██████████| 49/49 [00:04<00:00, 11.59it/s, epoch=151/200, avg_loss=0.101]\n",
      "100%|██████████| 49/49 [00:02<00:00, 18.01it/s, epoch=152/200, avg_loss=0.102] \n",
      "100%|██████████| 49/49 [00:02<00:00, 17.18it/s, epoch=153/200, avg_loss=0.102] \n",
      "100%|██████████| 49/49 [00:04<00:00, 10.55it/s, epoch=154/200, avg_loss=0.0998]\n",
      "100%|██████████| 49/49 [00:03<00:00, 14.09it/s, epoch=155/200, avg_loss=0.1]  \n",
      "100%|██████████| 49/49 [00:04<00:00, 11.46it/s, epoch=156/200, avg_loss=0.101] \n",
      "100%|██████████| 49/49 [00:03<00:00, 12.86it/s, epoch=157/200, avg_loss=0.0994]\n",
      "100%|██████████| 49/49 [00:04<00:00, 11.42it/s, epoch=158/200, avg_loss=0.0993]\n",
      "100%|██████████| 49/49 [00:02<00:00, 22.06it/s, epoch=159/200, avg_loss=0.101]\n",
      "100%|██████████| 49/49 [00:03<00:00, 16.10it/s, epoch=160/200, avg_loss=0.0996]\n",
      "100%|██████████| 49/49 [00:02<00:00, 16.62it/s, epoch=161/200, avg_loss=0.0965]\n",
      "100%|██████████| 49/49 [00:02<00:00, 19.44it/s, epoch=162/200, avg_loss=0.0996]\n",
      "100%|██████████| 49/49 [00:02<00:00, 19.21it/s, epoch=163/200, avg_loss=0.0994]\n",
      "100%|██████████| 49/49 [00:02<00:00, 16.72it/s, epoch=164/200, avg_loss=0.101]\n",
      "100%|██████████| 49/49 [00:02<00:00, 21.66it/s, epoch=165/200, avg_loss=0.1]  \n",
      "100%|██████████| 49/49 [00:03<00:00, 12.73it/s, epoch=166/200, avg_loss=0.1]  \n",
      "100%|██████████| 49/49 [00:03<00:00, 14.19it/s, epoch=167/200, avg_loss=0.0985]\n",
      "100%|██████████| 49/49 [00:04<00:00, 11.82it/s, epoch=168/200, avg_loss=0.0984]\n",
      "100%|██████████| 49/49 [00:02<00:00, 18.15it/s, epoch=169/200, avg_loss=0.0989]\n",
      "100%|██████████| 49/49 [00:02<00:00, 16.92it/s, epoch=170/200, avg_loss=0.0982]\n",
      "100%|██████████| 49/49 [00:02<00:00, 16.97it/s, epoch=171/200, avg_loss=0.0994]\n",
      "100%|██████████| 49/49 [00:02<00:00, 18.85it/s, epoch=172/200, avg_loss=0.1]   \n",
      "100%|██████████| 49/49 [00:03<00:00, 13.32it/s, epoch=173/200, avg_loss=0.0983]\n",
      "100%|██████████| 49/49 [00:02<00:00, 17.46it/s, epoch=174/200, avg_loss=0.0991]\n",
      "100%|██████████| 49/49 [00:03<00:00, 14.02it/s, epoch=175/200, avg_loss=0.0992]\n",
      "100%|██████████| 49/49 [00:02<00:00, 18.83it/s, epoch=176/200, avg_loss=0.101] \n",
      "100%|██████████| 49/49 [00:02<00:00, 21.82it/s, epoch=177/200, avg_loss=0.0983]\n",
      "100%|██████████| 49/49 [00:03<00:00, 15.65it/s, epoch=178/200, avg_loss=0.098] \n",
      "100%|██████████| 49/49 [00:04<00:00, 10.47it/s, epoch=179/200, avg_loss=0.099] \n",
      "100%|██████████| 49/49 [00:02<00:00, 16.40it/s, epoch=180/200, avg_loss=0.1]   \n",
      "100%|██████████| 49/49 [00:03<00:00, 13.42it/s, epoch=181/200, avg_loss=0.1]  \n",
      "100%|██████████| 49/49 [00:02<00:00, 18.18it/s, epoch=182/200, avg_loss=0.0973]\n",
      "100%|██████████| 49/49 [00:03<00:00, 12.54it/s, epoch=183/200, avg_loss=0.0979]\n",
      "100%|██████████| 49/49 [00:02<00:00, 21.72it/s, epoch=184/200, avg_loss=0.0976]\n",
      "100%|██████████| 49/49 [00:03<00:00, 13.56it/s, epoch=185/200, avg_loss=0.0966]\n",
      "100%|██████████| 49/49 [00:03<00:00, 12.62it/s, epoch=186/200, avg_loss=0.0981]\n",
      "100%|██████████| 49/49 [00:03<00:00, 14.44it/s, epoch=187/200, avg_loss=0.0984]\n",
      "100%|██████████| 49/49 [00:04<00:00, 11.38it/s, epoch=188/200, avg_loss=0.101] \n",
      "100%|██████████| 49/49 [00:02<00:00, 17.85it/s, epoch=189/200, avg_loss=0.097] \n",
      "100%|██████████| 49/49 [00:04<00:00, 12.25it/s, epoch=190/200, avg_loss=0.0981]\n",
      "100%|██████████| 49/49 [00:04<00:00, 12.00it/s, epoch=191/200, avg_loss=0.0973]\n",
      "100%|██████████| 49/49 [00:02<00:00, 17.01it/s, epoch=192/200, avg_loss=0.0967]\n",
      "100%|██████████| 49/49 [00:03<00:00, 12.78it/s, epoch=193/200, avg_loss=0.0966]\n",
      "100%|██████████| 49/49 [00:03<00:00, 12.94it/s, epoch=194/200, avg_loss=0.0968]\n",
      "100%|██████████| 49/49 [00:03<00:00, 12.64it/s, epoch=195/200, avg_loss=0.0973]\n",
      "100%|██████████| 49/49 [00:05<00:00,  8.71it/s, epoch=196/200, avg_loss=0.0953]\n",
      "100%|██████████| 49/49 [00:05<00:00,  9.53it/s, epoch=197/200, avg_loss=0.0963]\n",
      "100%|██████████| 49/49 [00:03<00:00, 14.81it/s, epoch=198/200, avg_loss=0.0963]\n",
      "100%|██████████| 49/49 [00:03<00:00, 13.38it/s, epoch=199/200, avg_loss=0.0965]\n",
      "100%|██████████| 49/49 [00:03<00:00, 12.66it/s, epoch=200/200, avg_loss=0.0978]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>gradient_norm</td><td>█▄▃▁▅▃▇▄▂▂▂▁▂▃▆▂▄▃▂▆▁▅▁▆▃▄▇▆▂▂██▇▇▅▅▇▃▆▄</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>██▅▆▅▄▅▃▂▄▃▃▃▃▃▂▂▂▂▂▂▁▂▂▁▂▂▁▂▂▂▂▂▁▁▂▁▂▁▁</td></tr><tr><td>training_time_per_epoch</td><td>▁▄▄▃▅▄▃█▂▂▂▃▃▄▃▂▄▂▃▄▃▄▁▄▃▂▂▁▃▁▃▃▂▃▂▂▁▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>200</td></tr><tr><td>gradient_norm</td><td>0.15365</td></tr><tr><td>learning_rate</td><td>0.0004</td></tr><tr><td>train_loss</td><td>0.09782</td></tr><tr><td>training_time_per_epoch</td><td>3.87108</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">brisk-surf-44</strong> at: <a href='https://wandb.ai/fyp_a/test_train_class/runs/cfzd1lxn/workspace' target=\"_blank\">https://wandb.ai/fyp_a/test_train_class/runs/cfzd1lxn/workspace</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240428_183539-cfzd1lxn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_path = 'C:/Users/Alexia/datasets/train_set_5class.pth'\n",
    "dataset = torch.load(file_path)\n",
    "epochs = 200\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=custom_collate_fn)\n",
    "net = GaussianDiffusionClass(EpsilonThetaClass(), input_size = 256)\n",
    "\n",
    "model_name = f'reshid16_resch16_batch{64}_lr{0.001}_e{epochs}'\n",
    "\n",
    "# Initialize and configure wandb run\n",
    "wandb.init(project=\"test_train_class\", name=model_name, reinit=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    net = net,\n",
    "    epochs=epochs,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.001,\n",
    "    model_name=model_name,\n",
    ")\n",
    "trainer(train_loader)\n",
    "\n",
    "# Ensure the current wandb run is properly closed before the next\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oneddif",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
